2025-10-06 20:15:27,485 - __main__ - INFO - Received signal 2, initiating graceful shutdown...
2025-10-09 08:15:19,007 - data.clinical_trial_rag - INFO - Building index from H:\Documents\LLM Code\AMP_LLMs\ct_database
2025-10-09 08:15:19,025 - data.clinical_trial_rag - INFO - Indexed 1 clinical trials
2025-10-09 08:15:47,646 - data.clinical_trial_rag - INFO - Building index from H:\Documents\LLM Code\AMP_LLMs\ct_database
2025-10-09 08:15:47,648 - data.clinical_trial_rag - INFO - Indexed 1 clinical trials
2025-10-09 08:26:07,364 - data.clinical_trial_rag - INFO - Building index from H:\Documents\LLM Code\AMP_LLMs\ct_database
2025-10-09 08:26:07,365 - data.clinical_trial_rag - INFO - Indexed 1 clinical trials
2025-10-09 08:26:23,225 - core.app - INFO - Connected to 100.99.162.98
2025-10-09 08:26:26,754 - asyncssh - INFO - Host canonicalization disabled
2025-10-09 08:26:26,754 - asyncssh - INFO - Opening SSH connection to 100.99.162.98, port 22
2025-10-09 08:26:26,859 - asyncssh - INFO - [conn=0] Connected to SSH server at 100.99.162.98, port 22
2025-10-09 08:26:26,859 - asyncssh - INFO - [conn=0]   Local address: 100.69.245.72, port 50296
2025-10-09 08:26:26,859 - asyncssh - INFO - [conn=0]   Peer address: 100.99.162.98, port 22
2025-10-09 08:26:26,951 - asyncssh - INFO - [conn=0] Beginning auth for user emilyzhang
2025-10-09 08:26:27,150 - asyncssh - INFO - [conn=0] Auth for user emilyzhang succeeded
2025-10-09 08:26:27,151 - core.app - INFO - SSH connection established: emilyzhang@100.99.162.98
2025-10-09 08:26:30,109 - core.menu - INFO - User selected: LLM Workflow (API Mode)
2025-10-09 08:26:30,110 - llm.async_llm_runner_api - INFO - Starting LLM workflow in API mode
2025-10-09 08:26:30,111 - llm.async_llm_utils - INFO - Fetching models from http://100.99.162.98:11434/api/tags
2025-10-09 08:26:32,393 - llm.async_llm_utils - ERROR - Cannot connect to http://100.99.162.98:11434/api/tags - is Ollama running and accessible?
2025-10-09 08:26:32,394 - asyncssh - INFO - [conn=0] Creating local TCP forwarder from port 11434 to localhost, port 11434
2025-10-09 08:26:32,396 - llm.async_llm_runner_api - INFO - SSH tunnel established
2025-10-09 08:26:33,405 - llm.async_llm_utils - INFO - Fetching models from http://localhost:11434/api/tags
2025-10-09 08:26:33,409 - asyncssh - INFO - [conn=0] Opening direct TCP connection to localhost, port 11434
2025-10-09 08:26:33,410 - asyncssh - INFO - [conn=0]   Client address: ::1, port 50302
2025-10-09 08:26:33,536 - llm.async_llm_utils - INFO - Found 13 models via API
2025-10-09 08:26:33,542 - asyncssh - INFO - [conn=0, chan=0] Received channel close
2025-10-09 08:26:33,543 - asyncssh - INFO - [conn=0, chan=0] Closing channel
2025-10-09 08:26:33,543 - asyncssh - INFO - [conn=0, chan=0] Channel closed
2025-10-09 08:27:13,118 - asyncssh - INFO - [conn=0] Connection failure: [WinError 121] The semaphore timeout period has expired
2025-10-09 08:27:19,231 - llm.async_llm_utils - INFO - Sending 4 characters to llama3:8b
2025-10-09 08:27:21,539 - llm.async_llm_utils - ERROR - Cannot connect to http://localhost:11434/api/generate
2025-10-09 08:27:24,102 - llm.async_llm_runner_api - INFO - SSH tunnel closed
2025-10-09 08:27:25,261 - core.menu - INFO - User selected: LLM Workflow (API Mode)
2025-10-09 08:27:25,261 - core.app - WARNING - SSH connection lost, attempting reconnect...
2025-10-09 08:27:25,263 - asyncssh - INFO - [conn=0] Closing connection
2025-10-09 08:27:25,263 - asyncssh - INFO - [conn=0] Sending disconnect: Disconnected by application (11)
2025-10-09 08:27:26,302 - asyncssh - INFO - Host canonicalization disabled
2025-10-09 08:27:26,302 - asyncssh - INFO - Opening SSH connection to 100.99.162.98, port 22
2025-10-09 08:27:26,319 - asyncssh - INFO - [conn=1] Connected to SSH server at 100.99.162.98, port 22
2025-10-09 08:27:26,319 - asyncssh - INFO - [conn=1]   Local address: 100.69.245.72, port 50310
2025-10-09 08:27:26,320 - asyncssh - INFO - [conn=1]   Peer address: 100.99.162.98, port 22
2025-10-09 08:27:26,404 - asyncssh - INFO - [conn=1] Beginning auth for user emilyzhang
2025-10-09 08:27:26,433 - asyncssh - INFO - [conn=1] Auth failed for user emilyzhang
2025-10-09 08:27:26,433 - asyncssh - INFO - [conn=1] Connection failure: Permission denied for user emilyzhang on host 100.99.162.98
2025-10-09 08:27:26,434 - asyncssh - INFO - [conn=1] Aborting connection
2025-10-09 08:27:29,929 - asyncssh - INFO - Host canonicalization disabled
2025-10-09 08:27:29,929 - asyncssh - INFO - Opening SSH connection to 100.99.162.98, port 22
2025-10-09 08:27:29,937 - asyncssh - INFO - [conn=2] Connected to SSH server at 100.99.162.98, port 22
2025-10-09 08:27:29,938 - asyncssh - INFO - [conn=2]   Local address: 100.69.245.72, port 50311
2025-10-09 08:27:29,938 - asyncssh - INFO - [conn=2]   Peer address: 100.99.162.98, port 22
2025-10-09 08:27:30,024 - asyncssh - INFO - [conn=2] Beginning auth for user emilyzhang
2025-10-09 08:27:30,224 - asyncssh - INFO - [conn=2] Auth for user emilyzhang succeeded
2025-10-09 08:27:30,225 - core.app - INFO - SSH connection established: emilyzhang@100.99.162.98
2025-10-09 08:27:30,226 - llm.async_llm_runner_api - INFO - Starting LLM workflow in API mode
2025-10-09 08:27:30,227 - llm.async_llm_utils - INFO - Fetching models from http://100.99.162.98:11434/api/tags
2025-10-09 08:27:32,411 - llm.async_llm_utils - ERROR - Cannot connect to http://100.99.162.98:11434/api/tags - is Ollama running and accessible?
2025-10-09 08:27:32,412 - asyncssh - INFO - [conn=2] Creating local TCP forwarder from port 11434 to localhost, port 11434
2025-10-09 08:27:32,414 - llm.async_llm_runner_api - INFO - SSH tunnel established
2025-10-09 08:27:33,417 - llm.async_llm_utils - INFO - Fetching models from http://localhost:11434/api/tags
2025-10-09 08:27:33,419 - asyncssh - INFO - [conn=2] Opening direct TCP connection to localhost, port 11434
2025-10-09 08:27:33,419 - asyncssh - INFO - [conn=2]   Client address: ::1, port 50313
2025-10-09 08:27:33,538 - llm.async_llm_utils - INFO - Found 13 models via API
2025-10-09 08:27:33,544 - asyncssh - INFO - [conn=2, chan=0] Received channel close
2025-10-09 08:27:33,545 - asyncssh - INFO - [conn=2, chan=0] Closing channel
2025-10-09 08:27:33,545 - asyncssh - INFO - [conn=2, chan=0] Channel closed
2025-10-09 08:27:41,749 - llm.async_llm_utils - INFO - Sending 13043 characters to llama3:8b
2025-10-09 08:27:41,751 - asyncssh - INFO - [conn=2] Opening direct TCP connection to localhost, port 11434
2025-10-09 08:27:41,752 - asyncssh - INFO - [conn=2]   Client address: ::1, port 50316
2025-10-09 08:28:18,213 - llm.async_llm_utils - INFO - Received response: 1857 characters
2025-10-09 08:28:18,221 - asyncssh - INFO - [conn=2, chan=1] Received channel close
2025-10-09 08:28:18,221 - asyncssh - INFO - [conn=2, chan=1] Closing channel
2025-10-09 08:28:18,222 - asyncssh - INFO - [conn=2, chan=1] Channel closed
2025-10-09 08:28:27,214 - llm.async_llm_runner_api - INFO - SSH tunnel closed
2025-10-09 08:28:28,436 - core.menu - INFO - User selected: Clinical Trial Research Assistant
2025-10-09 08:28:43,046 - data.clinical_trial_rag - INFO - Building index from H:\Documents\LLM Code\AMP_LLMs\output\nctload.txt
2025-10-09 08:28:43,046 - data.clinical_trial_rag - INFO - Indexed 1 clinical trials
2025-10-09 08:28:43,048 - llm.async_llm_utils - INFO - Fetching models from http://100.99.162.98:11434/api/tags
2025-10-09 08:28:45,201 - llm.async_llm_utils - ERROR - Cannot connect to http://100.99.162.98:11434/api/tags - is Ollama running and accessible?
2025-10-09 08:28:45,202 - asyncssh - INFO - [conn=2] Creating local TCP forwarder from port 11434 to localhost, port 11434
2025-10-09 08:28:46,207 - llm.async_llm_utils - INFO - Fetching models from http://localhost:11434/api/tags
2025-10-09 08:28:46,209 - asyncssh - INFO - [conn=2] Opening direct TCP connection to localhost, port 11434
2025-10-09 08:28:46,209 - asyncssh - INFO - [conn=2]   Client address: ::1, port 50326
2025-10-09 08:28:46,347 - llm.async_llm_utils - INFO - Found 13 models via API
2025-10-09 08:28:46,355 - asyncssh - INFO - [conn=2, chan=2] Received channel close
2025-10-09 08:28:46,356 - asyncssh - INFO - [conn=2, chan=2] Closing channel
2025-10-09 08:28:46,356 - asyncssh - INFO - [conn=2, chan=2] Channel closed
2025-10-09 08:28:51,455 - asyncssh - INFO - [conn=2, chan=3] Requesting new SSH session
2025-10-09 08:28:51,475 - asyncssh - INFO - [conn=2, chan=3]   Subsystem: sftp
2025-10-09 08:28:51,485 - asyncssh.sftp - INFO - [conn=2, chan=3] Starting SFTP client
2025-10-09 08:28:51,550 - asyncssh - INFO - [conn=2, chan=3] Received exit status 0
2025-10-09 08:28:51,550 - asyncssh - INFO - [conn=2, chan=3] Received channel close
2025-10-09 08:28:51,550 - asyncssh.sftp - INFO - [conn=2, chan=3] SFTP client exited
2025-10-09 08:28:51,550 - asyncssh - INFO - [conn=2, chan=3] Closing channel
2025-10-09 08:28:51,551 - asyncssh - INFO - [conn=2, chan=3] Channel closed
2025-10-09 08:28:51,552 - asyncssh - INFO - [conn=2, chan=4] Requesting new SSH session
2025-10-09 08:28:51,563 - asyncssh - INFO - [conn=2, chan=4]   Command: bash -l -c "ollama create ct-research-assistant -f /tmp/ct_modelfile_1760023731.modelfile"
2025-10-09 08:28:51,676 - asyncssh - INFO - [conn=2, chan=4] Received exit status 0
2025-10-09 08:28:51,676 - asyncssh - INFO - [conn=2, chan=4] Received channel close
2025-10-09 08:28:51,676 - asyncssh - INFO - [conn=2, chan=4] Channel closed
2025-10-09 08:28:51,677 - asyncssh - INFO - [conn=2, chan=5] Requesting new SSH session
2025-10-09 08:28:51,690 - asyncssh - INFO - [conn=2, chan=5]   Command: rm -f /tmp/ct_modelfile_1760023731.modelfile
2025-10-09 08:28:51,716 - asyncssh - INFO - [conn=2, chan=5] Received exit status 0
2025-10-09 08:28:51,716 - asyncssh - INFO - [conn=2, chan=5] Received channel close
2025-10-09 08:28:51,717 - asyncssh - INFO - [conn=2, chan=5] Channel closed
2025-10-09 08:29:13,816 - asyncssh - INFO - [conn=2, chan=6] Requesting new SSH session
2025-10-09 08:29:13,822 - asyncssh - INFO - [conn=2, chan=6]   Subsystem: sftp
2025-10-09 08:29:13,828 - asyncssh.sftp - INFO - [conn=2, chan=6] Starting SFTP client
2025-10-09 08:29:13,888 - asyncssh.sftp - INFO - [conn=2, chan=6] SFTP client exited
2025-10-09 08:29:13,888 - asyncssh - INFO - [conn=2, chan=6] Closing channel
2025-10-09 08:29:13,889 - asyncssh - INFO - [conn=2, chan=6] Received exit status 0
2025-10-09 08:29:13,889 - asyncssh - INFO - [conn=2, chan=6] Received channel close
2025-10-09 08:29:13,890 - asyncssh - INFO - [conn=2, chan=6] Channel closed
2025-10-09 08:29:13,891 - asyncssh - INFO - [conn=2, chan=7] Requesting new SSH session
2025-10-09 08:29:13,898 - asyncssh - INFO - [conn=2, chan=7]   Command: bash -l -c "ollama create ct-research-assistant -f /tmp/ct_modelfile_1760023753.modelfile"
2025-10-09 08:29:13,999 - asyncssh - INFO - [conn=2, chan=7] Received exit status 0
2025-10-09 08:29:14,000 - asyncssh - INFO - [conn=2, chan=7] Received channel close
2025-10-09 08:29:14,000 - asyncssh - INFO - [conn=2, chan=7] Channel closed
2025-10-09 08:29:14,000 - asyncssh - INFO - [conn=2, chan=8] Requesting new SSH session
2025-10-09 08:29:14,014 - asyncssh - INFO - [conn=2, chan=8]   Command: rm -f /tmp/ct_modelfile_1760023753.modelfile
2025-10-09 08:29:14,034 - asyncssh - INFO - [conn=2, chan=8] Received exit status 0
2025-10-09 08:29:14,035 - asyncssh - INFO - [conn=2, chan=8] Received channel close
2025-10-09 08:29:14,035 - asyncssh - INFO - [conn=2, chan=8] Channel closed
2025-10-09 08:29:42,455 - llm.async_llm_utils - INFO - Sending 1466 characters to ct-research-assistant
2025-10-09 08:29:42,459 - asyncssh - INFO - [conn=2] Opening direct TCP connection to localhost, port 11434
2025-10-09 08:29:42,460 - asyncssh - INFO - [conn=2]   Client address: ::1, port 50332
2025-10-09 08:30:03,026 - llm.async_llm_utils - INFO - Received response: 1178 characters
2025-10-09 08:30:03,034 - asyncssh - INFO - [conn=2, chan=9] Received channel close
2025-10-09 08:30:03,035 - asyncssh - INFO - [conn=2, chan=9] Closing channel
2025-10-09 08:30:03,035 - asyncssh - INFO - [conn=2, chan=9] Channel closed
2025-10-09 08:34:42,678 - asyncssh - INFO - [conn=2] Connection failure: [WinError 121] The semaphore timeout period has expired
2025-10-09 08:34:58,572 - core.app - INFO - Received signal 2, initiating graceful shutdown...
2025-10-09 08:34:58,573 - core.app - INFO - Closing SSH connection...
2025-10-09 08:34:58,574 - asyncssh - INFO - [conn=2] Closing connection
2025-10-09 08:34:58,574 - asyncssh - INFO - [conn=2] Sending disconnect: Disconnected by application (11)
2025-10-09 08:35:39,515 - core.app - INFO - Connected to 100.99.162.98
2025-10-09 08:35:43,057 - asyncssh - INFO - Host canonicalization disabled
2025-10-09 08:35:43,057 - asyncssh - INFO - Opening SSH connection to 100.99.162.98, port 22
2025-10-09 08:35:43,094 - asyncssh - INFO - [conn=0] Connected to SSH server at 100.99.162.98, port 22
2025-10-09 08:35:43,094 - asyncssh - INFO - [conn=0]   Local address: 100.69.245.72, port 50397
2025-10-09 08:35:43,095 - asyncssh - INFO - [conn=0]   Peer address: 100.99.162.98, port 22
2025-10-09 08:35:43,183 - asyncssh - INFO - [conn=0] Beginning auth for user emilyzhang
2025-10-09 08:35:43,390 - asyncssh - INFO - [conn=0] Auth for user emilyzhang succeeded
2025-10-09 08:35:43,390 - core.app - INFO - SSH connection established: emilyzhang@100.99.162.98
2025-10-09 08:35:45,324 - core.menu - INFO - User selected: Clinical Trial Research Assistant
2025-10-09 08:35:59,982 - core.menu - INFO - User selected: Clinical Trial Research Assistant
2025-10-09 08:36:00,973 - data.clinical_trial_rag - INFO - Building index from H:\Documents\LLM Code\AMP_LLMs\output\nctload.txt
2025-10-09 08:36:00,974 - data.clinical_trial_rag - INFO - Indexed 1 clinical trials
2025-10-09 08:36:00,975 - llm.async_llm_utils - INFO - Fetching models from http://100.99.162.98:11434/api/tags
2025-10-09 08:36:03,184 - llm.async_llm_utils - ERROR - Cannot connect to http://100.99.162.98:11434/api/tags - is Ollama running and accessible?
2025-10-09 08:36:03,185 - asyncssh - INFO - [conn=0] Creating local TCP forwarder from port 11434 to localhost, port 11434
2025-10-09 08:36:04,190 - llm.async_llm_utils - INFO - Fetching models from http://localhost:11434/api/tags
2025-10-09 08:36:04,195 - asyncssh - INFO - [conn=0] Opening direct TCP connection to localhost, port 11434
2025-10-09 08:36:04,195 - asyncssh - INFO - [conn=0]   Client address: ::1, port 50400
2025-10-09 08:36:04,313 - llm.async_llm_utils - INFO - Found 13 models via API
2025-10-09 08:36:04,321 - asyncssh - INFO - [conn=0, chan=0] Received channel close
2025-10-09 08:36:04,321 - asyncssh - INFO - [conn=0, chan=0] Closing channel
2025-10-09 08:36:04,322 - asyncssh - INFO - [conn=0, chan=0] Channel closed
2025-10-09 08:36:11,663 - asyncssh - INFO - [conn=0, chan=1] Requesting new SSH session
2025-10-09 08:36:11,818 - asyncssh - INFO - [conn=0, chan=1]   Subsystem: sftp
2025-10-09 08:36:11,827 - asyncssh.sftp - INFO - [conn=0, chan=1] Starting SFTP client
2025-10-09 08:36:12,012 - asyncssh - INFO - [conn=0, chan=1] Received exit status 0
2025-10-09 08:36:12,012 - asyncssh - INFO - [conn=0, chan=1] Received channel close
2025-10-09 08:36:12,013 - asyncssh.sftp - INFO - [conn=0, chan=1] SFTP client exited
2025-10-09 08:36:12,013 - asyncssh - INFO - [conn=0, chan=1] Closing channel
2025-10-09 08:36:12,013 - asyncssh - INFO - [conn=0, chan=1] Channel closed
2025-10-09 08:36:12,015 - asyncssh - INFO - [conn=0, chan=2] Requesting new SSH session
2025-10-09 08:36:12,023 - asyncssh - INFO - [conn=0, chan=2]   Command: bash -l -c "ollama create ct-research-assistant -f /tmp/ct_modelfile_1760024171.modelfile"
2025-10-09 08:36:12,130 - asyncssh - INFO - [conn=0, chan=2] Received exit status 0
2025-10-09 08:36:12,130 - asyncssh - INFO - [conn=0, chan=2] Received channel close
2025-10-09 08:36:12,130 - asyncssh - INFO - [conn=0, chan=2] Channel closed
2025-10-09 08:36:12,131 - asyncssh - INFO - [conn=0, chan=3] Requesting new SSH session
2025-10-09 08:36:12,137 - asyncssh - INFO - [conn=0, chan=3]   Command: rm -f /tmp/ct_modelfile_1760024171.modelfile
2025-10-09 08:36:12,208 - asyncssh - INFO - [conn=0, chan=3] Received exit status 0
2025-10-09 08:36:12,208 - asyncssh - INFO - [conn=0, chan=3] Received channel close
2025-10-09 08:36:12,209 - asyncssh - INFO - [conn=0, chan=3] Channel closed
2025-10-09 08:36:12,211 - llm.ct_research_runner - INFO - Created model ct-research-assistant from llama3:8b
2025-10-09 08:36:24,375 - llm.async_llm_utils - INFO - Sending 6804 characters to ct-research-assistant
2025-10-09 08:36:24,377 - asyncssh - INFO - [conn=0] Opening direct TCP connection to localhost, port 11434
2025-10-09 08:36:24,377 - asyncssh - INFO - [conn=0]   Client address: ::1, port 50402
2025-10-09 08:36:45,584 - llm.async_llm_utils - INFO - Received response: 618 characters
2025-10-09 08:36:45,591 - asyncssh - INFO - [conn=0, chan=4] Received channel close
2025-10-09 08:36:45,591 - asyncssh - INFO - [conn=0, chan=4] Closing channel
2025-10-09 08:36:45,591 - asyncssh - INFO - [conn=0, chan=4] Channel closed
2025-10-09 08:43:11,676 - asyncssh - INFO - [conn=0] Connection failure: [WinError 121] The semaphore timeout period has expired
2025-10-09 08:43:55,718 - llm.async_llm_utils - INFO - Sending 6804 characters to ct-research-assistant
2025-10-09 08:43:58,021 - llm.async_llm_utils - ERROR - Cannot connect to http://localhost:11434/api/generate
2025-10-09 08:44:02,096 - core.menu - INFO - User selected: Clinical Trial Research Assistant
2025-10-09 08:44:02,097 - core.app - WARNING - SSH connection lost, attempting reconnect...
2025-10-09 08:44:02,098 - asyncssh - INFO - [conn=0] Closing connection
2025-10-09 08:44:02,098 - asyncssh - INFO - [conn=0] Sending disconnect: Disconnected by application (11)
2025-10-09 08:44:04,861 - asyncssh - INFO - Host canonicalization disabled
2025-10-09 08:44:04,861 - asyncssh - INFO - Opening SSH connection to 100.99.162.98, port 22
2025-10-09 08:44:04,957 - asyncssh - INFO - [conn=1] Connected to SSH server at 100.99.162.98, port 22
2025-10-09 08:44:04,957 - asyncssh - INFO - [conn=1]   Local address: 100.69.245.72, port 50465
2025-10-09 08:44:04,957 - asyncssh - INFO - [conn=1]   Peer address: 100.99.162.98, port 22
2025-10-09 08:44:05,043 - asyncssh - INFO - [conn=1] Beginning auth for user emilyzhang
2025-10-09 08:44:05,222 - asyncssh - INFO - [conn=1] Auth for user emilyzhang succeeded
2025-10-09 08:44:05,223 - core.app - INFO - SSH connection established: emilyzhang@100.99.162.98
2025-10-09 08:44:08,796 - data.clinical_trial_rag - INFO - Building index from H:\Documents\LLM Code\AMP_LLMs\output\nctload.txt
2025-10-09 08:44:08,796 - data.clinical_trial_rag - INFO - Indexed 1 clinical trials
2025-10-09 08:44:08,798 - llm.async_llm_utils - INFO - Fetching models from http://100.99.162.98:11434/api/tags
2025-10-09 08:44:11,116 - llm.async_llm_utils - ERROR - Cannot connect to http://100.99.162.98:11434/api/tags - is Ollama running and accessible?
2025-10-09 08:44:11,117 - asyncssh - INFO - [conn=1] Creating local TCP forwarder from port 11434 to localhost, port 11434
2025-10-09 08:44:12,126 - llm.async_llm_utils - INFO - Fetching models from http://localhost:11434/api/tags
2025-10-09 08:44:12,127 - asyncssh - INFO - [conn=1] Opening direct TCP connection to localhost, port 11434
2025-10-09 08:44:12,128 - asyncssh - INFO - [conn=1]   Client address: ::1, port 50467
2025-10-09 08:44:12,155 - llm.async_llm_utils - INFO - Found 13 models via API
2025-10-09 08:44:12,161 - asyncssh - INFO - [conn=1, chan=0] Received channel close
2025-10-09 08:44:12,162 - asyncssh - INFO - [conn=1, chan=0] Closing channel
2025-10-09 08:44:12,162 - asyncssh - INFO - [conn=1, chan=0] Channel closed
2025-10-09 08:44:15,544 - asyncssh - INFO - [conn=1, chan=1] Requesting new SSH session
2025-10-09 08:44:15,608 - asyncssh - INFO - [conn=1, chan=1]   Subsystem: sftp
2025-10-09 08:44:15,615 - asyncssh.sftp - INFO - [conn=1, chan=1] Starting SFTP client
2025-10-09 08:44:15,669 - asyncssh - INFO - [conn=1, chan=1] Received exit status 0
2025-10-09 08:44:15,669 - asyncssh - INFO - [conn=1, chan=1] Received channel close
2025-10-09 08:44:15,670 - asyncssh.sftp - INFO - [conn=1, chan=1] SFTP client exited
2025-10-09 08:44:15,670 - asyncssh - INFO - [conn=1, chan=1] Closing channel
2025-10-09 08:44:15,670 - asyncssh - INFO - [conn=1, chan=1] Channel closed
2025-10-09 08:44:15,672 - asyncssh - INFO - [conn=1, chan=2] Requesting new SSH session
2025-10-09 08:44:15,679 - asyncssh - INFO - [conn=1, chan=2]   Command: bash -l -c "ollama create ct-research-assistant -f /tmp/ct_modelfile_1760024655.modelfile"
2025-10-09 08:44:15,783 - asyncssh - INFO - [conn=1, chan=2] Received exit status 0
2025-10-09 08:44:15,783 - asyncssh - INFO - [conn=1, chan=2] Received channel close
2025-10-09 08:44:15,783 - asyncssh - INFO - [conn=1, chan=2] Channel closed
2025-10-09 08:44:15,784 - asyncssh - INFO - [conn=1, chan=3] Requesting new SSH session
2025-10-09 08:44:15,790 - asyncssh - INFO - [conn=1, chan=3]   Command: rm -f /tmp/ct_modelfile_1760024655.modelfile
2025-10-09 08:44:15,817 - asyncssh - INFO - [conn=1, chan=3] Received exit status 0
2025-10-09 08:44:15,818 - asyncssh - INFO - [conn=1, chan=3] Received channel close
2025-10-09 08:44:15,818 - asyncssh - INFO - [conn=1, chan=3] Channel closed
2025-10-09 08:44:15,821 - llm.ct_research_runner - INFO - Created model ct-research-assistant from llama3:8b
2025-10-09 08:44:19,222 - llm.async_llm_utils - INFO - Sending 6804 characters to ct-research-assistant
2025-10-09 08:44:19,223 - asyncssh - INFO - [conn=1] Opening direct TCP connection to localhost, port 11434
2025-10-09 08:44:19,224 - asyncssh - INFO - [conn=1]   Client address: ::1, port 50469
2025-10-09 08:44:41,972 - llm.async_llm_utils - INFO - Received response: 783 characters
2025-10-09 08:44:41,980 - asyncssh - INFO - [conn=1, chan=4] Received channel close
2025-10-09 08:44:41,980 - asyncssh - INFO - [conn=1, chan=4] Closing channel
2025-10-09 08:44:41,980 - asyncssh - INFO - [conn=1, chan=4] Channel closed
2025-10-09 08:45:00,219 - llm.ct_research_runner - INFO - Saved extraction to output\NCT04043065_extraction.json
2025-10-09 08:51:27,163 - asyncssh - INFO - [conn=1] Connection failure: [WinError 121] The semaphore timeout period has expired
2025-10-09 09:24:22,616 - core.app - INFO - Received signal 2, initiating graceful shutdown...
2025-10-09 09:24:22,618 - core.app - INFO - Closing SSH connection...
2025-10-09 09:24:22,618 - asyncssh - INFO - [conn=1] Closing connection
2025-10-09 09:24:22,618 - asyncssh - INFO - [conn=1] Sending disconnect: Disconnected by application (11)
2025-10-09 10:01:48,901 - core.app - INFO - Connected to 100.99.162.98
2025-10-09 10:01:53,325 - asyncssh - INFO - Host canonicalization disabled
2025-10-09 10:01:53,326 - asyncssh - INFO - Opening SSH connection to 100.99.162.98, port 22
2025-10-09 10:01:53,390 - asyncssh - INFO - [conn=0] Connected to SSH server at 100.99.162.98, port 22
2025-10-09 10:01:53,390 - asyncssh - INFO - [conn=0]   Local address: 100.69.245.72, port 51183
2025-10-09 10:01:53,390 - asyncssh - INFO - [conn=0]   Peer address: 100.99.162.98, port 22
2025-10-09 10:01:53,484 - asyncssh - INFO - [conn=0] Beginning auth for user emilyzhang
2025-10-09 10:01:55,530 - asyncssh - INFO - [conn=0] Auth failed for user emilyzhang
2025-10-09 10:01:55,530 - asyncssh - INFO - [conn=0] Connection failure: Permission denied for user emilyzhang on host 100.99.162.98
2025-10-09 10:01:55,530 - asyncssh - INFO - [conn=0] Aborting connection
2025-10-09 10:01:59,677 - asyncssh - INFO - Host canonicalization disabled
2025-10-09 10:01:59,677 - asyncssh - INFO - Opening SSH connection to 100.99.162.98, port 22
2025-10-09 10:01:59,743 - asyncssh - INFO - [conn=1] Connected to SSH server at 100.99.162.98, port 22
2025-10-09 10:01:59,743 - asyncssh - INFO - [conn=1]   Local address: 100.69.245.72, port 51188
2025-10-09 10:01:59,743 - asyncssh - INFO - [conn=1]   Peer address: 100.99.162.98, port 22
2025-10-09 10:01:59,829 - asyncssh - INFO - [conn=1] Beginning auth for user emilyzhang
2025-10-09 10:02:00,014 - asyncssh - INFO - [conn=1] Auth for user emilyzhang succeeded
2025-10-09 10:02:00,015 - core.app - INFO - SSH connection established: emilyzhang@100.99.162.98
2025-10-09 10:02:02,651 - core.menu - INFO - User selected: NCT Lookup
2025-10-09 10:02:20,420 - data.async_nct_lookup - INFO - Fetching data for: NCT07013110
2025-10-09 10:02:23,984 - data.async_nct_lookup - INFO - Successfully fetched complete data for NCT07013110
2025-10-09 10:02:55,796 - data.async_nct_lookup - INFO - Fetching data for: NCT07013110
2025-10-09 10:02:59,548 - data.api_clients - INFO - OpenFDA returned 10 events for Placebo
2025-10-09 10:02:59,551 - data.async_nct_lookup - INFO - Successfully fetched complete data for NCT07013110
2025-10-09 10:03:10,820 - data.async_nct_lookup - INFO - Fetching data for: NCT07013110
2025-10-09 10:03:14,460 - data.api_clients - WARNING - DuckDuckGo library not available
2025-10-09 10:03:14,462 - data.async_nct_lookup - INFO - Successfully fetched complete data for NCT07013110
2025-10-09 10:03:23,673 - data.async_nct_lookup - INFO - Saved 1 results to duckduckgo.json
2025-10-09 10:03:52,972 - data.async_nct_lookup - INFO - Fetching data for: NCT07013110
2025-10-09 10:03:57,341 - data.api_clients - INFO - SERP API Scholar returned 0 results
2025-10-09 10:03:58,294 - data.api_clients - INFO - SERP API returned 1 results
2025-10-09 10:03:58,296 - data.async_nct_lookup - INFO - Successfully fetched complete data for NCT07013110
2025-10-09 10:04:11,022 - data.async_nct_lookup - INFO - Saved 1 results to google.json
2025-10-09 10:05:32,916 - data.async_nct_lookup - INFO - Fetching data for: NCT07013110
2025-10-09 10:05:36,823 - data.api_clients - INFO - OpenFDA returned 10 events for Placebo
2025-10-09 10:05:36,826 - data.async_nct_lookup - INFO - Successfully fetched complete data for NCT07013110
2025-10-09 10:05:47,746 - data.async_nct_lookup - INFO - Saved 1 results to openfda.json
2025-10-09 10:06:20,065 - core.menu - INFO - User selected: Clinical Trial Research Assistant
2025-10-09 10:06:26,212 - data.clinical_trial_rag - INFO - Building index from H:\Documents\LLM Code\AMP_LLMs\output\openfda.json
2025-10-09 10:06:26,217 - data.clinical_trial_rag - INFO - Indexed 1 clinical trials
2025-10-09 10:06:26,218 - llm.async_llm_utils - INFO - Fetching models from http://100.99.162.98:11434/api/tags
2025-10-09 10:06:28,537 - llm.async_llm_utils - ERROR - Cannot connect to http://100.99.162.98:11434/api/tags - is Ollama running and accessible?
2025-10-09 10:06:28,538 - asyncssh - INFO - [conn=1] Creating local TCP forwarder from port 11434 to localhost, port 11434
2025-10-09 10:06:29,539 - llm.async_llm_utils - INFO - Fetching models from http://localhost:11434/api/tags
2025-10-09 10:06:29,541 - asyncssh - INFO - [conn=1] Opening direct TCP connection to localhost, port 11434
2025-10-09 10:06:29,541 - asyncssh - INFO - [conn=1]   Client address: ::1, port 51281
2025-10-09 10:06:29,591 - llm.async_llm_utils - INFO - Found 13 models via API
2025-10-09 10:06:29,598 - asyncssh - INFO - [conn=1, chan=0] Received channel close
2025-10-09 10:06:29,598 - asyncssh - INFO - [conn=1, chan=0] Closing channel
2025-10-09 10:06:29,598 - asyncssh - INFO - [conn=1, chan=0] Channel closed
2025-10-09 10:06:36,421 - core.app - INFO - Received signal 2, initiating graceful shutdown...
2025-10-09 10:06:36,422 - core.app - INFO - Closing SSH connection...
2025-10-09 10:06:36,422 - asyncssh - INFO - [conn=1] Closing connection
2025-10-09 10:06:36,422 - asyncssh - INFO - [conn=1] Sending disconnect: Disconnected by application (11)
2025-10-09 10:06:36,422 - asyncssh - INFO - [conn=1] Connection closed
