2025-10-09 10:26:42,223 - data.clinical_trial_rag - INFO - Building index from H:\Documents\LLM Code\AMP_LLMs\ct_database
2025-10-09 10:26:42,235 - data.clinical_trial_rag - INFO - Indexed 1 clinical trials
2025-10-09 10:27:38,076 - data.clinical_trial_rag - INFO - Building index from H:\Documents\LLM Code\AMP_LLMs\ct_database
2025-10-09 10:27:38,080 - data.clinical_trial_rag - INFO - Indexed 1 clinical trials
2025-10-09 10:38:08,990 - core.app - WARNING - Failed to ping 100.99.162.98
2025-10-09 10:38:10,490 - core.app - WARNING - Failed to ping 100.99.162.98
2025-10-09 10:39:02,754 - core.app - INFO - Connected to 100.99.162.98
2025-10-09 10:39:06,563 - asyncssh - INFO - Host canonicalization disabled
2025-10-09 10:39:06,563 - asyncssh - INFO - Opening SSH connection to 100.99.162.98, port 22
2025-10-09 10:39:06,636 - asyncssh - INFO - [conn=0] Connected to SSH server at 100.99.162.98, port 22
2025-10-09 10:39:06,636 - asyncssh - INFO - [conn=0]   Local address: 100.69.245.72, port 51667
2025-10-09 10:39:06,636 - asyncssh - INFO - [conn=0]   Peer address: 100.99.162.98, port 22
2025-10-09 10:39:06,733 - asyncssh - INFO - [conn=0] Beginning auth for user emilyzhang
2025-10-09 10:39:06,929 - asyncssh - INFO - [conn=0] Auth for user emilyzhang succeeded
2025-10-09 10:39:06,930 - core.app - INFO - SSH connection established: emilyzhang@100.99.162.98
2025-10-09 10:39:09,432 - core.menu - INFO - User selected: Clinical Trial Research Assistant
2025-10-09 10:39:15,337 - data.clinical_trial_rag - INFO - Building index from H:\Documents\LLM Code\AMP_LLMs\output\openfda.json
2025-10-09 10:39:15,342 - data.clinical_trial_rag - INFO - Indexed 1 clinical trials
2025-10-09 10:39:15,344 - llm.async_llm_utils - INFO - Fetching models from http://100.99.162.98:11434/api/tags
2025-10-09 10:39:17,549 - llm.async_llm_utils - ERROR - Cannot connect to http://100.99.162.98:11434/api/tags - is Ollama running and accessible?
2025-10-09 10:39:17,550 - asyncssh - INFO - [conn=0] Creating local TCP forwarder from port 11434 to localhost, port 11434
2025-10-09 10:39:18,565 - llm.async_llm_utils - INFO - Fetching models from http://localhost:11434/api/tags
2025-10-09 10:39:18,569 - asyncssh - INFO - [conn=0] Opening direct TCP connection to localhost, port 11434
2025-10-09 10:39:18,569 - asyncssh - INFO - [conn=0]   Client address: ::1, port 51671
2025-10-09 10:39:18,730 - llm.async_llm_utils - INFO - Found 13 models via API
2025-10-09 10:39:18,737 - asyncssh - INFO - [conn=0, chan=0] Received channel close
2025-10-09 10:39:18,737 - asyncssh - INFO - [conn=0, chan=0] Closing channel
2025-10-09 10:39:18,738 - asyncssh - INFO - [conn=0, chan=0] Channel closed
2025-10-09 10:39:26,635 - asyncssh - INFO - [conn=0, chan=1] Requesting new SSH session
2025-10-09 10:39:26,706 - asyncssh - INFO - [conn=0, chan=1]   Subsystem: sftp
2025-10-09 10:39:26,712 - asyncssh.sftp - INFO - [conn=0, chan=1] Starting SFTP client
2025-10-09 10:39:26,780 - asyncssh.sftp - INFO - [conn=0, chan=1] SFTP client exited
2025-10-09 10:39:26,780 - asyncssh - INFO - [conn=0, chan=1] Closing channel
2025-10-09 10:39:26,781 - asyncssh - INFO - [conn=0, chan=1] Received exit status 0
2025-10-09 10:39:26,781 - asyncssh - INFO - [conn=0, chan=1] Received channel close
2025-10-09 10:39:26,782 - asyncssh - INFO - [conn=0, chan=1] Channel closed
2025-10-09 10:39:26,784 - asyncssh - INFO - [conn=0, chan=2] Requesting new SSH session
2025-10-09 10:39:26,790 - asyncssh - INFO - [conn=0, chan=2]   Command: bash -l -c "ollama create ct-research-assistant -f /tmp/ct_modelfile_1760031566.modelfile"
2025-10-09 10:39:26,880 - asyncssh - INFO - [conn=0, chan=2] Received exit status 0
2025-10-09 10:39:26,880 - asyncssh - INFO - [conn=0, chan=2] Received channel close
2025-10-09 10:39:26,881 - asyncssh - INFO - [conn=0, chan=2] Channel closed
2025-10-09 10:39:26,881 - asyncssh - INFO - [conn=0, chan=3] Requesting new SSH session
2025-10-09 10:39:26,887 - asyncssh - INFO - [conn=0, chan=3]   Command: rm -f /tmp/ct_modelfile_1760031566.modelfile
2025-10-09 10:39:26,903 - asyncssh - INFO - [conn=0, chan=3] Received exit status 0
2025-10-09 10:39:26,904 - asyncssh - INFO - [conn=0, chan=3] Received channel close
2025-10-09 10:39:26,904 - asyncssh - INFO - [conn=0, chan=3] Channel closed
2025-10-09 10:39:26,906 - llm.ct_research_runner - INFO - Created model ct-research-assistant from llama2:7b
2025-10-09 10:39:38,129 - llm.ct_research_runner - ERROR - Error in research assistant: 'ClinicalTrialResearchAssistant' object has no attribute 'query_with_rag'
Traceback (most recent call last):
  File "h:\Documents\LLM Code\AMP_LLMs\llm\ct_research_runner.py", line 624, in run_ct_research_assistant
    response = await assistant.query_with_rag(host, args)
                     ^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ClinicalTrialResearchAssistant' object has no attribute 'query_with_rag'
2025-10-09 10:42:21,856 - core.app - INFO - Received signal 2, initiating graceful shutdown...
2025-10-09 10:42:21,858 - core.app - INFO - Closing SSH connection...
2025-10-09 10:42:21,858 - asyncssh - INFO - [conn=0] Closing connection
2025-10-09 10:42:21,858 - asyncssh - INFO - [conn=0] Sending disconnect: Disconnected by application (11)
2025-10-09 10:42:21,859 - asyncssh - INFO - [conn=0] Connection closed
2025-10-09 10:42:30,007 - core.app - INFO - Connected to 100.99.162.98
2025-10-09 10:42:33,809 - asyncssh - INFO - Host canonicalization disabled
2025-10-09 10:42:33,809 - asyncssh - INFO - Opening SSH connection to 100.99.162.98, port 22
2025-10-09 10:42:33,933 - asyncssh - INFO - [conn=0] Connected to SSH server at 100.99.162.98, port 22
2025-10-09 10:42:33,933 - asyncssh - INFO - [conn=0]   Local address: 100.69.245.72, port 51705
2025-10-09 10:42:33,934 - asyncssh - INFO - [conn=0]   Peer address: 100.99.162.98, port 22
2025-10-09 10:42:34,331 - asyncssh - INFO - [conn=0] Beginning auth for user emilyzhang
2025-10-09 10:42:34,637 - asyncssh - INFO - [conn=0] Auth for user emilyzhang succeeded
2025-10-09 10:42:34,638 - core.app - INFO - SSH connection established: emilyzhang@100.99.162.98
2025-10-09 10:42:36,086 - core.menu - INFO - User selected: Clinical Trial Research Assistant
2025-10-09 10:42:48,576 - data.clinical_trial_rag - INFO - Building index from H:\Documents\LLM Code\AMP_LLMs\output\openfda.json
2025-10-09 10:42:48,582 - data.clinical_trial_rag - INFO - Indexed 1 clinical trials
2025-10-09 10:42:48,583 - llm.async_llm_utils - INFO - Fetching models from http://100.99.162.98:11434/api/tags
2025-10-09 10:42:51,005 - llm.async_llm_utils - ERROR - Cannot connect to http://100.99.162.98:11434/api/tags - is Ollama running and accessible?
2025-10-09 10:42:51,006 - asyncssh - INFO - [conn=0] Creating local TCP forwarder from port 11434 to localhost, port 11434
2025-10-09 10:42:52,009 - llm.async_llm_utils - INFO - Fetching models from http://localhost:11434/api/tags
2025-10-09 10:42:52,013 - asyncssh - INFO - [conn=0] Opening direct TCP connection to localhost, port 11434
2025-10-09 10:42:52,014 - asyncssh - INFO - [conn=0]   Client address: ::1, port 51708
2025-10-09 10:42:52,144 - llm.async_llm_utils - INFO - Found 13 models via API
2025-10-09 10:42:52,151 - asyncssh - INFO - [conn=0, chan=0] Received channel close
2025-10-09 10:42:52,152 - asyncssh - INFO - [conn=0, chan=0] Closing channel
2025-10-09 10:42:52,152 - asyncssh - INFO - [conn=0, chan=0] Channel closed
2025-10-09 10:42:59,210 - asyncssh - INFO - [conn=0, chan=1] Requesting new SSH session
2025-10-09 10:42:59,252 - asyncssh - INFO - [conn=0, chan=1]   Subsystem: sftp
2025-10-09 10:42:59,260 - asyncssh.sftp - INFO - [conn=0, chan=1] Starting SFTP client
2025-10-09 10:42:59,324 - asyncssh - INFO - [conn=0, chan=1] Received exit status 0
2025-10-09 10:42:59,325 - asyncssh - INFO - [conn=0, chan=1] Received channel close
2025-10-09 10:42:59,325 - asyncssh.sftp - INFO - [conn=0, chan=1] SFTP client exited
2025-10-09 10:42:59,325 - asyncssh - INFO - [conn=0, chan=1] Closing channel
2025-10-09 10:42:59,325 - asyncssh - INFO - [conn=0, chan=1] Channel closed
2025-10-09 10:42:59,327 - asyncssh - INFO - [conn=0, chan=2] Requesting new SSH session
2025-10-09 10:42:59,335 - asyncssh - INFO - [conn=0, chan=2]   Command: bash -l -c "ollama create ct-research-assistant -f /tmp/ct_modelfile_1760031779.modelfile"
2025-10-09 10:42:59,416 - asyncssh - INFO - [conn=0, chan=2] Received exit status 0
2025-10-09 10:42:59,417 - asyncssh - INFO - [conn=0, chan=2] Received channel close
2025-10-09 10:42:59,417 - asyncssh - INFO - [conn=0, chan=2] Channel closed
2025-10-09 10:42:59,417 - asyncssh - INFO - [conn=0, chan=3] Requesting new SSH session
2025-10-09 10:42:59,425 - asyncssh - INFO - [conn=0, chan=3]   Command: rm -f /tmp/ct_modelfile_1760031779.modelfile
2025-10-09 10:42:59,444 - asyncssh - INFO - [conn=0, chan=3] Received exit status 0
2025-10-09 10:42:59,445 - asyncssh - INFO - [conn=0, chan=3] Received channel close
2025-10-09 10:42:59,445 - asyncssh - INFO - [conn=0, chan=3] Channel closed
2025-10-09 10:42:59,447 - llm.ct_research_runner - INFO - Created model ct-research-assistant from llama2:7b
2025-10-09 10:43:04,033 - llm.ct_research_runner - ERROR - Error in research assistant: 'ClinicalTrialResearchAssistant' object has no attribute 'query_with_rag'
Traceback (most recent call last):
  File "h:\Documents\LLM Code\AMP_LLMs\llm\ct_research_runner.py", line 621, in run_ct_research_assistant
    response = await assistant.query_with_rag(host, args)
                     ^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ClinicalTrialResearchAssistant' object has no attribute 'query_with_rag'
2025-10-09 10:43:28,643 - asyncssh - INFO - [conn=0] Connection failure: [WinError 121] The semaphore timeout period has expired
2025-10-09 10:44:01,152 - llm.ct_research_runner - ERROR - Error in research assistant: 'ClinicalTrialResearchAssistant' object has no attribute 'extract_from_nct'
Traceback (most recent call last):
  File "h:\Documents\LLM Code\AMP_LLMs\llm\ct_research_runner.py", line 516, in run_ct_research_assistant
    response = await assistant.extract_from_nct(host, nct)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ClinicalTrialResearchAssistant' object has no attribute 'extract_from_nct'
2025-10-09 10:53:37,088 - llm.ct_research_runner - ERROR - Error in research assistant: 'ClinicalTrialResearchAssistant' object has no attribute 'query_with_rag'
Traceback (most recent call last):
  File "h:\Documents\LLM Code\AMP_LLMs\llm\ct_research_runner.py", line 800, in run_ct_research_assistant
    response = await assistant.query_with_rag(host, user_input)
                     ^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ClinicalTrialResearchAssistant' object has no attribute 'query_with_rag'
2025-10-09 10:54:54,998 - core.app - INFO - Received signal 2, initiating graceful shutdown...
2025-10-09 10:54:54,999 - core.app - INFO - Closing SSH connection...
2025-10-09 10:54:55,000 - asyncssh - INFO - [conn=0] Closing connection
2025-10-09 10:54:55,000 - asyncssh - INFO - [conn=0] Sending disconnect: Disconnected by application (11)
2025-10-09 10:55:03,230 - core.app - INFO - Connected to 100.99.162.98
2025-10-09 10:55:06,592 - asyncssh - INFO - Host canonicalization disabled
2025-10-09 10:55:06,592 - asyncssh - INFO - Opening SSH connection to 100.99.162.98, port 22
2025-10-09 10:55:06,641 - asyncssh - INFO - [conn=0] Connected to SSH server at 100.99.162.98, port 22
2025-10-09 10:55:06,641 - asyncssh - INFO - [conn=0]   Local address: 100.69.245.72, port 51810
2025-10-09 10:55:06,642 - asyncssh - INFO - [conn=0]   Peer address: 100.99.162.98, port 22
2025-10-09 10:55:06,844 - asyncssh - INFO - [conn=0] Beginning auth for user emilyzhang
2025-10-09 10:55:07,170 - asyncssh - INFO - [conn=0] Auth for user emilyzhang succeeded
2025-10-09 10:55:07,171 - core.app - INFO - SSH connection established: emilyzhang@100.99.162.98
2025-10-09 10:55:08,366 - core.menu - INFO - User selected: Clinical Trial Research Assistant
2025-10-09 10:55:12,895 - data.clinical_trial_rag - INFO - Building index from H:\Documents\LLM Code\AMP_LLMs\output\openfda.json
2025-10-09 10:55:12,900 - data.clinical_trial_rag - INFO - Indexed 1 clinical trials
2025-10-09 10:55:12,902 - llm.async_llm_utils - INFO - Fetching models from http://100.99.162.98:11434/api/tags
2025-10-09 10:55:15,348 - llm.async_llm_utils - ERROR - Cannot connect to http://100.99.162.98:11434/api/tags - is Ollama running and accessible?
2025-10-09 10:55:15,349 - asyncssh - INFO - [conn=0] Creating local TCP forwarder from port 11434 to localhost, port 11434
2025-10-09 10:55:16,356 - llm.async_llm_utils - INFO - Fetching models from http://localhost:11434/api/tags
2025-10-09 10:55:16,360 - asyncssh - INFO - [conn=0] Opening direct TCP connection to localhost, port 11434
2025-10-09 10:55:16,361 - asyncssh - INFO - [conn=0]   Client address: ::1, port 51814
2025-10-09 10:55:16,388 - llm.async_llm_utils - INFO - Found 13 models via API
2025-10-09 10:55:20,760 - llm.ct_research_runner - INFO - Created model ct-research-assistant from llama2:7b
2025-10-09 10:55:32,428 - llm.async_llm_utils - INFO - Sending 1773 characters to ct-research-assistant
2025-10-09 10:55:32,430 - asyncssh - INFO - [conn=0] Opening direct TCP connection to localhost, port 11434
2025-10-09 10:55:32,430 - asyncssh - INFO - [conn=0]   Client address: ::1, port 51815
2025-10-09 10:55:56,739 - asyncssh - INFO - [conn=0] Connection failure: [WinError 121] The semaphore timeout period has expired
2025-10-09 10:55:56,740 - asyncssh - INFO - [conn=0, chan=4] Closing channel due to connection close
2025-10-09 10:55:56,740 - asyncssh - INFO - [conn=0, chan=4] Closing channel
2025-10-09 10:55:56,740 - asyncssh - INFO - [conn=0, chan=4] Channel closed: [WinError 121] The semaphore timeout period has expired
2025-10-09 10:55:56,741 - llm.async_llm_utils - ERROR - Error sending prompt: Server disconnected
Traceback (most recent call last):
  File "h:\Documents\LLM Code\AMP_LLMs\llm\async_llm_utils.py", line 98, in send_to_ollama_api
    async with session.post(url, json=payload, timeout=aiohttp.ClientTimeout(total=120)) as resp:
               ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Documents\LLM Code\AMP_LLMs\llm_env\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "H:\Documents\LLM Code\AMP_LLMs\llm_env\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "H:\Documents\LLM Code\AMP_LLMs\llm_env\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "H:\Documents\LLM Code\AMP_LLMs\llm_env\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Documents\LLM Code\AMP_LLMs\llm_env\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.ServerDisconnectedError: Server disconnected
2025-10-09 10:56:12,513 - llm.async_llm_utils - INFO - Sending 1773 characters to ct-research-assistant
2025-10-09 10:56:14,805 - llm.async_llm_utils - ERROR - Cannot connect to http://localhost:11434/api/generate
2025-10-09 10:56:16,918 - core.app - INFO - Received signal 2, initiating graceful shutdown...
2025-10-09 10:56:16,919 - core.app - INFO - Closing SSH connection...
2025-10-09 10:56:16,919 - asyncssh - INFO - [conn=0] Closing connection
2025-10-09 10:56:16,919 - asyncssh - INFO - [conn=0] Sending disconnect: Disconnected by application (11)
2025-10-09 10:56:24,210 - core.app - INFO - Connected to 100.99.162.98
2025-10-09 10:56:27,264 - asyncssh - INFO - Host canonicalization disabled
2025-10-09 10:56:27,264 - asyncssh - INFO - Opening SSH connection to 100.99.162.98, port 22
2025-10-09 10:56:27,270 - asyncssh - INFO - [conn=0] Connected to SSH server at 100.99.162.98, port 22
2025-10-09 10:56:27,271 - asyncssh - INFO - [conn=0]   Local address: 100.69.245.72, port 51829
2025-10-09 10:56:27,271 - asyncssh - INFO - [conn=0]   Peer address: 100.99.162.98, port 22
2025-10-09 10:56:27,349 - asyncssh - INFO - [conn=0] Beginning auth for user emilyzhang
2025-10-09 10:56:27,561 - asyncssh - INFO - [conn=0] Auth for user emilyzhang succeeded
2025-10-09 10:56:27,562 - core.app - INFO - SSH connection established: emilyzhang@100.99.162.98
2025-10-09 10:56:28,622 - core.menu - INFO - User selected: Clinical Trial Research Assistant
2025-10-09 10:56:32,599 - data.clinical_trial_rag - INFO - Building index from H:\Documents\LLM Code\AMP_LLMs\output\openfda.json
2025-10-09 10:56:32,607 - data.clinical_trial_rag - INFO - Indexed 1 clinical trials
2025-10-09 10:56:32,608 - llm.async_llm_utils - INFO - Fetching models from http://100.99.162.98:11434/api/tags
2025-10-09 10:56:34,893 - llm.async_llm_utils - ERROR - Cannot connect to http://100.99.162.98:11434/api/tags - is Ollama running and accessible?
2025-10-09 10:56:34,894 - asyncssh - INFO - [conn=0] Creating local TCP forwarder from port 11434 to localhost, port 11434
2025-10-09 10:56:35,897 - llm.async_llm_utils - INFO - Fetching models from http://localhost:11434/api/tags
2025-10-09 10:56:35,902 - asyncssh - INFO - [conn=0] Opening direct TCP connection to localhost, port 11434
2025-10-09 10:56:35,902 - asyncssh - INFO - [conn=0]   Client address: ::1, port 51832
2025-10-09 10:56:35,949 - llm.async_llm_utils - INFO - Found 13 models via API
2025-10-09 10:56:42,825 - llm.ct_research_runner - INFO - Created model ct-research-assistant from llama3:8b
2025-10-09 10:56:46,993 - llm.async_llm_utils - INFO - Sending 1773 characters to ct-research-assistant
2025-10-09 10:56:46,995 - asyncssh - INFO - [conn=0] Opening direct TCP connection to localhost, port 11434
2025-10-09 10:56:46,995 - asyncssh - INFO - [conn=0]   Client address: ::1, port 51833
2025-10-09 10:57:13,281 - llm.async_llm_utils - INFO - Received response: 1166 characters
2025-10-09 10:57:13,295 - asyncssh - INFO - [conn=0, chan=4] Received channel close
2025-10-09 10:57:13,296 - asyncssh - INFO - [conn=0, chan=4] Closing channel
2025-10-09 10:57:13,296 - asyncssh - INFO - [conn=0, chan=4] Channel closed
2025-10-09 10:57:56,523 - llm.async_llm_utils - INFO - Sending 272 characters to ct-research-assistant
2025-10-09 10:57:56,526 - asyncssh - INFO - [conn=0] Opening direct TCP connection to localhost, port 11434
2025-10-09 10:57:56,526 - asyncssh - INFO - [conn=0]   Client address: ::1, port 51849
2025-10-09 10:58:08,368 - llm.async_llm_utils - INFO - Received response: 876 characters
2025-10-09 10:58:08,376 - asyncssh - INFO - [conn=0, chan=5] Received channel close
2025-10-09 10:58:08,376 - asyncssh - INFO - [conn=0, chan=5] Closing channel
2025-10-09 10:58:08,377 - asyncssh - INFO - [conn=0, chan=5] Channel closed
2025-10-09 11:09:29,174 - core.app - INFO - Received signal 2, initiating graceful shutdown...
2025-10-09 11:09:29,175 - core.app - INFO - Closing SSH connection...
2025-10-09 11:09:29,176 - asyncssh - INFO - [conn=0] Closing connection
2025-10-09 11:09:29,176 - asyncssh - INFO - [conn=0] Sending disconnect: Disconnected by application (11)
2025-10-09 11:09:29,176 - asyncssh - INFO - [conn=0] Connection closed
2025-10-09 11:09:41,858 - core.app - INFO - Connected to 100.99.162.98
2025-10-09 11:09:45,702 - asyncssh - INFO - Host canonicalization disabled
2025-10-09 11:09:45,703 - asyncssh - INFO - Opening SSH connection to 100.99.162.98, port 22
2025-10-09 11:09:45,754 - asyncssh - INFO - [conn=0] Connected to SSH server at 100.99.162.98, port 22
2025-10-09 11:09:45,755 - asyncssh - INFO - [conn=0]   Local address: 100.69.245.72, port 51954
2025-10-09 11:09:45,755 - asyncssh - INFO - [conn=0]   Peer address: 100.99.162.98, port 22
2025-10-09 11:09:45,839 - asyncssh - INFO - [conn=0] Beginning auth for user emilyzhang
2025-10-09 11:09:46,048 - asyncssh - INFO - [conn=0] Auth for user emilyzhang succeeded
2025-10-09 11:09:46,049 - core.app - INFO - SSH connection established: emilyzhang@100.99.162.98
2025-10-09 11:09:48,613 - core.menu - INFO - User selected: NCT Lookup
2025-10-09 11:10:07,613 - data.async_nct_lookup - INFO - Fetching data for: NCT04043065
2025-10-09 11:10:12,014 - data.async_nct_lookup - INFO - Successfully fetched complete data for NCT04043065
2025-10-09 11:10:20,861 - data.async_nct_lookup - INFO - Fetching data for: NCT04043065
2025-10-09 11:10:25,498 - data.api_clients - INFO - OpenFDA returned 10 events for Placebo
2025-10-09 11:10:25,501 - data.async_nct_lookup - INFO - Successfully fetched complete data for NCT04043065
2025-10-09 11:11:42,587 - data.async_nct_lookup - INFO - Saved 1 results to openfdaresults.txt
2025-10-09 11:13:56,079 - data.async_nct_lookup - INFO - User returned to main menu from NCT lookup
2025-10-09 11:13:58,100 - core.menu - INFO - User selected: NCT Lookup
2025-10-09 11:14:26,355 - asyncssh - INFO - [conn=0] Server not responding to keepalive
2025-10-09 11:15:30,045 - data.async_nct_lookup - INFO - Fetching data for: NCT04043065
2025-10-09 11:15:34,930 - data.async_nct_lookup - INFO - Successfully fetched complete data for NCT04043065
2025-10-09 11:15:44,652 - data.async_nct_lookup - INFO - Fetching data for: NCT04043065
2025-10-09 11:15:49,264 - data.api_clients - INFO - OpenFDA returned 10 events for Placebo
2025-10-09 11:15:49,266 - data.async_nct_lookup - INFO - Successfully fetched complete data for NCT04043065
2025-10-09 11:17:41,844 - core.app - INFO - Received signal 2, initiating graceful shutdown...
2025-10-09 11:17:41,844 - core.app - INFO - Closing SSH connection...
2025-10-09 11:17:41,845 - asyncssh - INFO - [conn=0] Closing connection
2025-10-09 11:17:41,845 - asyncssh - INFO - [conn=0] Sending disconnect: Disconnected by application (11)
2025-10-09 11:17:49,510 - core.app - INFO - Connected to 100.99.162.98
2025-10-09 11:17:52,758 - asyncssh - INFO - Host canonicalization disabled
2025-10-09 11:17:52,758 - asyncssh - INFO - Opening SSH connection to 100.99.162.98, port 22
2025-10-09 11:17:52,878 - asyncssh - INFO - [conn=0] Connected to SSH server at 100.99.162.98, port 22
2025-10-09 11:17:52,878 - asyncssh - INFO - [conn=0]   Local address: 100.69.245.72, port 52052
2025-10-09 11:17:52,878 - asyncssh - INFO - [conn=0]   Peer address: 100.99.162.98, port 22
2025-10-09 11:17:52,958 - asyncssh - INFO - [conn=0] Beginning auth for user emilyzhang
2025-10-09 11:17:53,138 - asyncssh - INFO - [conn=0] Auth for user emilyzhang succeeded
2025-10-09 11:17:53,139 - core.app - INFO - SSH connection established: emilyzhang@100.99.162.98
2025-10-09 11:17:55,516 - core.menu - INFO - User selected: Clinical Trial Research Assistant
2025-10-09 11:18:01,477 - data.clinical_trial_rag - INFO - Building index from H:\Documents\LLM Code\AMP_LLMs\output\openfda.json
2025-10-09 11:18:01,491 - data.clinical_trial_rag - INFO - Indexed 1 clinical trials
2025-10-09 11:18:01,493 - llm.async_llm_utils - INFO - Fetching models from http://100.99.162.98:11434/api/tags
2025-10-09 11:18:03,923 - llm.async_llm_utils - ERROR - Cannot connect to http://100.99.162.98:11434/api/tags - is Ollama running and accessible?
2025-10-09 11:18:03,923 - asyncssh - INFO - [conn=0] Creating local TCP forwarder from port 11434 to localhost, port 11434
2025-10-09 11:18:04,938 - llm.async_llm_utils - INFO - Fetching models from http://localhost:11434/api/tags
2025-10-09 11:18:04,942 - asyncssh - INFO - [conn=0] Opening direct TCP connection to localhost, port 11434
2025-10-09 11:18:04,943 - asyncssh - INFO - [conn=0]   Client address: ::1, port 52056
2025-10-09 11:18:05,078 - llm.async_llm_utils - INFO - Found 13 models via API
2025-10-09 11:18:13,071 - llm.ct_research_runner - INFO - Created model ct-research-assistant from llama2:13b
2025-10-09 11:18:46,660 - core.menu - INFO - User selected: Clinical Trial Research Assistant
2025-10-09 11:18:53,389 - data.clinical_trial_rag - INFO - Building index from H:\Documents\LLM Code\AMP_LLMs\output\nctload.txt
2025-10-09 11:18:53,389 - data.clinical_trial_rag - INFO - Indexed 1 clinical trials
2025-10-09 11:18:53,390 - llm.async_llm_utils - INFO - Fetching models from http://100.99.162.98:11434/api/tags
2025-10-09 11:18:55,694 - llm.async_llm_utils - ERROR - Cannot connect to http://100.99.162.98:11434/api/tags - is Ollama running and accessible?
2025-10-09 11:18:55,695 - asyncssh - INFO - [conn=0] Creating local TCP forwarder from port 11434 to localhost, port 11434
2025-10-09 11:18:56,697 - llm.async_llm_utils - INFO - Fetching models from http://localhost:11434/api/tags
2025-10-09 11:18:56,703 - asyncssh - INFO - [conn=0] Opening direct TCP connection to localhost, port 11434
2025-10-09 11:18:56,703 - asyncssh - INFO - [conn=0]   Client address: ::1, port 52061
2025-10-09 11:18:56,734 - llm.async_llm_utils - INFO - Found 13 models via API
2025-10-09 11:19:01,502 - llm.ct_research_runner - INFO - Created model ct-research-assistant from llama2:7b
2025-10-09 11:19:12,773 - llm.async_llm_utils - INFO - Sending 1402 characters to ct-research-assistant
2025-10-09 11:19:12,775 - asyncssh - INFO - [conn=0] Opening direct TCP connection to localhost, port 11434
2025-10-09 11:19:12,775 - asyncssh - INFO - [conn=0]   Client address: ::1, port 52066
2025-10-09 11:19:46,566 - llm.async_llm_utils - INFO - Received response: 1016 characters
2025-10-09 11:19:46,580 - asyncssh - INFO - [conn=0, chan=8] Received channel close
2025-10-09 11:19:46,581 - asyncssh - INFO - [conn=0, chan=8] Closing channel
2025-10-09 11:19:46,581 - asyncssh - INFO - [conn=0, chan=8] Channel closed
2025-10-09 11:20:39,604 - llm.async_llm_utils - INFO - Sending 263 characters to ct-research-assistant
2025-10-09 11:20:39,606 - asyncssh - INFO - [conn=0] Opening direct TCP connection to localhost, port 11434
2025-10-09 11:20:39,606 - asyncssh - INFO - [conn=0]   Client address: ::1, port 52076
2025-10-09 11:20:46,686 - llm.async_llm_utils - INFO - Received response: 464 characters
2025-10-09 11:20:46,693 - asyncssh - INFO - [conn=0, chan=9] Received channel close
2025-10-09 11:20:46,693 - asyncssh - INFO - [conn=0, chan=9] Closing channel
2025-10-09 11:20:46,693 - asyncssh - INFO - [conn=0, chan=9] Channel closed
2025-10-09 11:21:01,003 - core.menu - INFO - User selected: NCT Lookup
2025-10-09 11:21:07,900 - data.async_nct_lookup - INFO - Fetching data for: NCT05168774
2025-10-09 11:21:15,246 - data.async_nct_lookup - INFO - Successfully fetched complete data for NCT05168774
2025-10-09 11:21:26,454 - data.async_nct_lookup - INFO - Saved 1 results to NCT05168774_method.json
2025-10-09 11:21:29,043 - core.menu - INFO - User selected: Clinical Trial Research Assistant
2025-10-09 11:21:36,756 - data.clinical_trial_rag - INFO - Building index from H:\Documents\LLM Code\AMP_LLMs\output\NCT05168774_method.json
2025-10-09 11:21:36,757 - data.clinical_trial_rag - INFO - Indexed 1 clinical trials
2025-10-09 11:21:36,758 - llm.async_llm_utils - INFO - Fetching models from http://100.99.162.98:11434/api/tags
2025-10-09 11:21:38,830 - llm.async_llm_utils - ERROR - Cannot connect to http://100.99.162.98:11434/api/tags - is Ollama running and accessible?
2025-10-09 11:21:38,831 - asyncssh - INFO - [conn=0] Creating local TCP forwarder from port 11434 to localhost, port 11434
2025-10-09 11:21:39,833 - llm.async_llm_utils - INFO - Fetching models from http://localhost:11434/api/tags
2025-10-09 11:21:39,837 - asyncssh - INFO - [conn=0] Opening direct TCP connection to localhost, port 11434
2025-10-09 11:21:39,838 - asyncssh - INFO - [conn=0]   Client address: ::1, port 52092
2025-10-09 11:21:39,891 - llm.async_llm_utils - INFO - Found 13 models via API
2025-10-09 11:21:49,995 - llm.ct_research_runner - INFO - Created model ct-research-assistant from llama3:8b
2025-10-09 11:22:02,180 - llm.async_llm_utils - INFO - Sending 1420 characters to ct-research-assistant
2025-10-09 11:22:02,182 - asyncssh - INFO - [conn=0] Opening direct TCP connection to localhost, port 11434
2025-10-09 11:22:02,182 - asyncssh - INFO - [conn=0]   Client address: ::1, port 52093
2025-10-09 11:22:40,433 - llm.async_llm_utils - INFO - Received response: 1010 characters
2025-10-09 11:22:40,447 - asyncssh - INFO - [conn=0, chan=14] Received channel close
2025-10-09 11:22:40,448 - asyncssh - INFO - [conn=0, chan=14] Closing channel
2025-10-09 11:22:40,448 - asyncssh - INFO - [conn=0, chan=14] Channel closed
2025-10-09 11:23:07,452 - llm.async_llm_utils - INFO - Sending 339 characters to ct-research-assistant
2025-10-09 11:23:07,454 - asyncssh - INFO - [conn=0] Opening direct TCP connection to localhost, port 11434
2025-10-09 11:23:07,455 - asyncssh - INFO - [conn=0]   Client address: ::1, port 52097
2025-10-09 11:23:11,706 - llm.async_llm_utils - INFO - Received response: 419 characters
2025-10-09 11:23:11,714 - asyncssh - INFO - [conn=0, chan=15] Received channel close
2025-10-09 11:23:11,714 - asyncssh - INFO - [conn=0, chan=15] Closing channel
2025-10-09 11:23:11,714 - asyncssh - INFO - [conn=0, chan=15] Channel closed
2025-10-09 11:23:39,684 - llm.async_llm_utils - INFO - Sending 262 characters to ct-research-assistant
2025-10-09 11:23:39,686 - asyncssh - INFO - [conn=0] Opening direct TCP connection to localhost, port 11434
2025-10-09 11:23:39,687 - asyncssh - INFO - [conn=0]   Client address: ::1, port 52103
2025-10-09 11:23:50,956 - asyncssh - INFO - [conn=0] Connection failure: [WinError 121] The semaphore timeout period has expired
2025-10-09 11:23:50,956 - asyncssh - INFO - [conn=0, chan=16] Closing channel due to connection close
2025-10-09 11:23:50,956 - asyncssh - INFO - [conn=0, chan=16] Channel closed: [WinError 121] The semaphore timeout period has expired
2025-10-09 11:23:50,957 - llm.async_llm_utils - ERROR - Error sending prompt: Server disconnected
Traceback (most recent call last):
  File "h:\Documents\LLM Code\AMP_LLMs\llm\async_llm_utils.py", line 98, in send_to_ollama_api
    async with session.post(url, json=payload, timeout=aiohttp.ClientTimeout(total=120)) as resp:
               ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Documents\LLM Code\AMP_LLMs\llm_env\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "H:\Documents\LLM Code\AMP_LLMs\llm_env\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "H:\Documents\LLM Code\AMP_LLMs\llm_env\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "H:\Documents\LLM Code\AMP_LLMs\llm_env\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Documents\LLM Code\AMP_LLMs\llm_env\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.ServerDisconnectedError: Server disconnected
2025-10-09 11:23:54,059 - core.app - INFO - Received signal 2, initiating graceful shutdown...
2025-10-09 11:23:54,060 - core.app - INFO - Closing SSH connection...
2025-10-09 11:23:54,061 - asyncssh - INFO - [conn=0] Closing connection
2025-10-09 11:23:54,061 - asyncssh - INFO - [conn=0] Sending disconnect: Disconnected by application (11)
2025-10-09 11:24:01,090 - core.app - INFO - Connected to 100.99.162.98
2025-10-09 11:24:05,373 - asyncssh - INFO - Host canonicalization disabled
2025-10-09 11:24:05,373 - asyncssh - INFO - Opening SSH connection to 100.99.162.98, port 22
2025-10-09 11:24:05,396 - asyncssh - INFO - [conn=0] Connected to SSH server at 100.99.162.98, port 22
2025-10-09 11:24:05,396 - asyncssh - INFO - [conn=0]   Local address: 100.69.245.72, port 52110
2025-10-09 11:24:05,396 - asyncssh - INFO - [conn=0]   Peer address: 100.99.162.98, port 22
2025-10-09 11:24:05,488 - asyncssh - INFO - [conn=0] Beginning auth for user emilyzhang
2025-10-09 11:24:05,678 - asyncssh - INFO - [conn=0] Auth for user emilyzhang succeeded
2025-10-09 11:24:05,679 - core.app - INFO - SSH connection established: emilyzhang@100.99.162.98
2025-10-09 11:24:08,123 - core.menu - INFO - User selected: Clinical Trial Research Assistant
2025-10-09 11:24:12,156 - data.clinical_trial_rag - INFO - Building index from H:\Documents\LLM Code\AMP_LLMs\output\NCT05168774_method.json
2025-10-09 11:24:12,157 - data.clinical_trial_rag - INFO - Indexed 1 clinical trials
2025-10-09 11:24:12,158 - llm.async_llm_utils - INFO - Fetching models from http://100.99.162.98:11434/api/tags
2025-10-09 11:24:14,624 - llm.async_llm_utils - ERROR - Cannot connect to http://100.99.162.98:11434/api/tags - is Ollama running and accessible?
2025-10-09 11:24:14,625 - asyncssh - INFO - [conn=0] Creating local TCP forwarder from port 11434 to localhost, port 11434
2025-10-09 11:24:15,630 - llm.async_llm_utils - INFO - Fetching models from http://localhost:11434/api/tags
2025-10-09 11:24:15,635 - asyncssh - INFO - [conn=0] Opening direct TCP connection to localhost, port 11434
2025-10-09 11:24:15,635 - asyncssh - INFO - [conn=0]   Client address: ::1, port 52117
2025-10-09 11:24:15,766 - llm.async_llm_utils - INFO - Found 13 models via API
2025-10-09 11:24:21,269 - llm.ct_research_runner - INFO - Created model ct-research-assistant from llama3:8b
2025-10-09 11:24:37,852 - llm.async_llm_utils - INFO - Sending 1472 characters to ct-research-assistant
2025-10-09 11:24:37,855 - asyncssh - INFO - [conn=0] Opening direct TCP connection to localhost, port 11434
2025-10-09 11:24:37,855 - asyncssh - INFO - [conn=0]   Client address: ::1, port 52121
2025-10-09 11:24:44,475 - llm.async_llm_utils - INFO - Received response: 394 characters
2025-10-09 11:24:44,482 - asyncssh - INFO - [conn=0, chan=4] Received channel close
2025-10-09 11:24:44,482 - asyncssh - INFO - [conn=0, chan=4] Closing channel
2025-10-09 11:24:44,483 - asyncssh - INFO - [conn=0, chan=4] Channel closed
2025-10-09 11:24:51,900 - llm.async_llm_utils - INFO - Sending 1420 characters to ct-research-assistant
2025-10-09 11:24:51,902 - asyncssh - INFO - [conn=0] Opening direct TCP connection to localhost, port 11434
2025-10-09 11:24:51,902 - asyncssh - INFO - [conn=0]   Client address: ::1, port 52123
2025-10-09 11:25:05,640 - llm.async_llm_utils - INFO - Received response: 801 characters
2025-10-09 11:25:05,655 - asyncssh - INFO - [conn=0, chan=5] Received channel close
2025-10-09 11:25:05,655 - asyncssh - INFO - [conn=0, chan=5] Closing channel
2025-10-09 11:25:05,656 - asyncssh - INFO - [conn=0, chan=5] Channel closed
2025-10-09 11:25:23,196 - llm.async_llm_utils - INFO - Sending 1427 characters to ct-research-assistant
2025-10-09 11:25:23,198 - asyncssh - INFO - [conn=0] Opening direct TCP connection to localhost, port 11434
2025-10-09 11:25:23,198 - asyncssh - INFO - [conn=0]   Client address: ::1, port 52127
2025-10-09 11:25:33,723 - llm.async_llm_utils - INFO - Received response: 656 characters
2025-10-09 11:25:33,732 - asyncssh - INFO - [conn=0, chan=6] Received channel close
2025-10-09 11:25:33,732 - asyncssh - INFO - [conn=0, chan=6] Closing channel
2025-10-09 11:25:33,732 - asyncssh - INFO - [conn=0, chan=6] Channel closed
2025-10-09 11:26:05,468 - llm.async_llm_utils - INFO - Sending 1467 characters to ct-research-assistant
2025-10-09 11:26:05,471 - asyncssh - INFO - [conn=0] Opening direct TCP connection to localhost, port 11434
2025-10-09 11:26:05,471 - asyncssh - INFO - [conn=0]   Client address: ::1, port 52129
2025-10-09 11:26:30,696 - llm.async_llm_utils - INFO - Received response: 1123 characters
2025-10-09 11:26:30,704 - asyncssh - INFO - [conn=0, chan=7] Received channel close
2025-10-09 11:26:30,705 - asyncssh - INFO - [conn=0, chan=7] Closing channel
2025-10-09 11:26:30,705 - asyncssh - INFO - [conn=0, chan=7] Channel closed
2025-10-09 11:26:56,732 - llm.async_llm_utils - INFO - Sending 327 characters to ct-research-assistant
2025-10-09 11:26:56,734 - asyncssh - INFO - [conn=0] Opening direct TCP connection to localhost, port 11434
2025-10-09 11:26:56,735 - asyncssh - INFO - [conn=0]   Client address: ::1, port 52135
2025-10-09 11:27:00,910 - llm.async_llm_utils - INFO - Received response: 414 characters
2025-10-09 11:27:00,918 - asyncssh - INFO - [conn=0, chan=8] Received channel close
2025-10-09 11:27:00,919 - asyncssh - INFO - [conn=0, chan=8] Closing channel
2025-10-09 11:27:00,919 - asyncssh - INFO - [conn=0, chan=8] Channel closed
2025-10-09 11:27:54,820 - llm.async_llm_utils - INFO - Sending 272 characters to ct-research-assistant
2025-10-09 11:27:54,822 - asyncssh - INFO - [conn=0] Opening direct TCP connection to localhost, port 11434
2025-10-09 11:27:54,822 - asyncssh - INFO - [conn=0]   Client address: ::1, port 52147
2025-10-09 11:28:00,562 - asyncssh - INFO - [conn=0] Connection failure: [WinError 121] The semaphore timeout period has expired
2025-10-09 11:28:00,562 - asyncssh - INFO - [conn=0, chan=9] Closing channel due to connection close
2025-10-09 11:28:00,562 - asyncssh - INFO - [conn=0, chan=9] Channel closed: [WinError 121] The semaphore timeout period has expired
2025-10-09 11:28:00,563 - llm.async_llm_utils - ERROR - Error sending prompt: Server disconnected
Traceback (most recent call last):
  File "h:\Documents\LLM Code\AMP_LLMs\llm\async_llm_utils.py", line 98, in send_to_ollama_api
    async with session.post(url, json=payload, timeout=aiohttp.ClientTimeout(total=120)) as resp:
               ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Documents\LLM Code\AMP_LLMs\llm_env\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "H:\Documents\LLM Code\AMP_LLMs\llm_env\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "H:\Documents\LLM Code\AMP_LLMs\llm_env\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "H:\Documents\LLM Code\AMP_LLMs\llm_env\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Documents\LLM Code\AMP_LLMs\llm_env\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.ServerDisconnectedError: Server disconnected
2025-10-09 11:28:05,211 - core.app - INFO - Received signal 2, initiating graceful shutdown...
2025-10-09 11:28:05,212 - core.app - INFO - Closing SSH connection...
2025-10-09 11:28:05,212 - asyncssh - INFO - [conn=0] Closing connection
2025-10-09 11:28:05,212 - asyncssh - INFO - [conn=0] Sending disconnect: Disconnected by application (11)
2025-10-09 13:14:29,470 - core.app - INFO - Connected to 100.99.162.98
2025-10-09 13:14:32,584 - asyncssh - INFO - Host canonicalization disabled
2025-10-09 13:14:32,585 - asyncssh - INFO - Opening SSH connection to 100.99.162.98, port 22
2025-10-09 13:14:32,593 - asyncssh - INFO - [conn=0] Connected to SSH server at 100.99.162.98, port 22
2025-10-09 13:14:32,593 - asyncssh - INFO - [conn=0]   Local address: 100.69.245.72, port 53103
2025-10-09 13:14:32,593 - asyncssh - INFO - [conn=0]   Peer address: 100.99.162.98, port 22
2025-10-09 13:14:32,679 - asyncssh - INFO - [conn=0] Beginning auth for user emilyzhang
2025-10-09 13:14:32,863 - asyncssh - INFO - [conn=0] Auth for user emilyzhang succeeded
2025-10-09 13:14:32,864 - core.app - INFO - SSH connection established: emilyzhang@100.99.162.98
2025-10-09 13:14:35,007 - core.menu - INFO - User selected: NCT Lookup
2025-10-09 13:14:47,951 - data.async_nct_lookup - INFO - Fetching data for: NCT05168774
2025-10-09 13:14:55,480 - data.async_nct_lookup - INFO - Successfully fetched complete data for NCT05168774
2025-10-09 13:15:17,210 - data.async_nct_lookup - INFO - Saved 1 results to NCT05168774_extended.json
2025-10-09 13:15:19,359 - core.menu - INFO - User selected: Clinical Trial Research Assistant
2025-10-09 13:15:27,043 - data.clinical_trial_rag - INFO - Building index from H:\Documents\LLM Code\AMP_LLMs\output\NCT05168774_extended.json
2025-10-09 13:15:27,044 - data.clinical_trial_rag - INFO - Indexed 1 clinical trials
2025-10-09 13:15:27,046 - llm.async_llm_utils - INFO - Fetching models from http://100.99.162.98:11434/api/tags
2025-10-09 13:15:29,330 - llm.async_llm_utils - ERROR - Cannot connect to http://100.99.162.98:11434/api/tags - is Ollama running and accessible?
2025-10-09 13:15:29,331 - asyncssh - INFO - [conn=0] Creating local TCP forwarder from port 11434 to localhost, port 11434
2025-10-09 13:15:30,340 - llm.async_llm_utils - INFO - Fetching models from http://localhost:11434/api/tags
2025-10-09 13:15:30,343 - asyncssh - INFO - [conn=0] Opening direct TCP connection to localhost, port 11434
2025-10-09 13:15:30,343 - asyncssh - INFO - [conn=0]   Client address: ::1, port 53125
2025-10-09 13:15:30,380 - llm.async_llm_utils - INFO - Found 13 models via API
2025-10-09 13:15:35,738 - llm.ct_research_runner - INFO - Created model ct-research-assistant from llama3:8b
2025-10-09 13:15:54,272 - llm.async_llm_utils - INFO - Sending 1420 characters to ct-research-assistant
2025-10-09 13:15:54,275 - asyncssh - INFO - [conn=0] Opening direct TCP connection to localhost, port 11434
2025-10-09 13:15:54,275 - asyncssh - INFO - [conn=0]   Client address: ::1, port 53130
2025-10-09 13:16:14,931 - llm.async_llm_utils - INFO - Received response: 1054 characters
2025-10-09 13:16:14,939 - asyncssh - INFO - [conn=0, chan=4] Received channel close
2025-10-09 13:16:14,939 - asyncssh - INFO - [conn=0, chan=4] Closing channel
2025-10-09 13:16:14,939 - asyncssh - INFO - [conn=0, chan=4] Channel closed
2025-10-09 13:17:00,118 - llm.async_llm_utils - INFO - Sending 282 characters to ct-research-assistant
2025-10-09 13:17:00,120 - asyncssh - INFO - [conn=0] Opening direct TCP connection to localhost, port 11434
2025-10-09 13:17:00,121 - asyncssh - INFO - [conn=0]   Client address: ::1, port 53136
2025-10-09 13:17:03,384 - llm.async_llm_utils - INFO - Received response: 314 characters
2025-10-09 13:17:03,393 - asyncssh - INFO - [conn=0, chan=5] Received channel close
2025-10-09 13:17:03,394 - asyncssh - INFO - [conn=0, chan=5] Closing channel
2025-10-09 13:17:03,394 - asyncssh - INFO - [conn=0, chan=5] Channel closed
2025-10-09 13:17:42,695 - asyncssh - INFO - [conn=0] Connection failure: [WinError 121] The semaphore timeout period has expired
2025-10-09 13:17:53,503 - llm.async_llm_utils - INFO - Sending 228 characters to ct-research-assistant
2025-10-09 13:17:55,807 - llm.async_llm_utils - ERROR - Cannot connect to http://localhost:11434/api/generate
2025-10-09 13:23:01,006 - core.app - INFO - Received signal 2, initiating graceful shutdown...
2025-10-09 13:23:01,007 - core.app - INFO - Closing SSH connection...
2025-10-09 13:23:01,007 - asyncssh - INFO - [conn=0] Closing connection
2025-10-09 13:23:01,008 - asyncssh - INFO - [conn=0] Sending disconnect: Disconnected by application (11)
2025-10-09 13:23:39,843 - core.app - INFO - Connected to 100.99.162.98
2025-10-09 13:23:43,979 - asyncssh - INFO - Host canonicalization disabled
2025-10-09 13:23:43,979 - asyncssh - INFO - Opening SSH connection to 100.99.162.98, port 22
2025-10-09 13:23:44,027 - asyncssh - INFO - [conn=0] Connected to SSH server at 100.99.162.98, port 22
2025-10-09 13:23:44,027 - asyncssh - INFO - [conn=0]   Local address: 100.69.245.72, port 53213
2025-10-09 13:23:44,028 - asyncssh - INFO - [conn=0]   Peer address: 100.99.162.98, port 22
2025-10-09 13:23:44,125 - asyncssh - INFO - [conn=0] Beginning auth for user emilyzhang
2025-10-09 13:23:44,331 - asyncssh - INFO - [conn=0] Auth for user emilyzhang succeeded
2025-10-09 13:23:44,332 - core.app - INFO - SSH connection established: emilyzhang@100.99.162.98
2025-10-09 13:23:44,453 - core.app - ERROR - Fatal error: unexpected indent (ct_research_runner.py, line 262)
Traceback (most recent call last):
  File "h:\Documents\LLM Code\AMP_LLMs\core\app.py", line 215, in run
    self.menu_system = MenuSystem(self)
                       ~~~~~~~~~~^^^^^^
  File "h:\Documents\LLM Code\AMP_LLMs\core\menu.py", line 46, in __init__
    self._register_menu_items()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "h:\Documents\LLM Code\AMP_LLMs\core\menu.py", line 55, in _register_menu_items
    from llm.ct_research_runner import run_ct_research_assistant
  File "h:\Documents\LLM Code\AMP_LLMs\llm\ct_research_runner.py", line 262
    if create.strip().lower() in ('n', 'no'):
IndentationError: unexpected indent
2025-10-09 13:23:44,475 - core.app - INFO - Closing SSH connection...
2025-10-09 13:23:44,475 - asyncssh - INFO - [conn=0] Closing connection
2025-10-09 13:23:44,476 - asyncssh - INFO - [conn=0] Sending disconnect: Disconnected by application (11)
2025-10-09 13:23:44,476 - asyncssh - INFO - [conn=0] Connection closed
2025-10-09 13:25:21,003 - core.app - INFO - Connected to 100.99.162.98
2025-10-09 13:25:24,648 - asyncssh - INFO - Host canonicalization disabled
2025-10-09 13:25:24,649 - asyncssh - INFO - Opening SSH connection to 100.99.162.98, port 22
2025-10-09 13:25:24,686 - asyncssh - INFO - [conn=0] Connected to SSH server at 100.99.162.98, port 22
2025-10-09 13:25:24,686 - asyncssh - INFO - [conn=0]   Local address: 100.69.245.72, port 53242
2025-10-09 13:25:24,686 - asyncssh - INFO - [conn=0]   Peer address: 100.99.162.98, port 22
2025-10-09 13:25:24,778 - asyncssh - INFO - [conn=0] Beginning auth for user emilyzhang
2025-10-09 13:25:24,962 - asyncssh - INFO - [conn=0] Auth for user emilyzhang succeeded
2025-10-09 13:25:24,963 - core.app - INFO - SSH connection established: emilyzhang@100.99.162.98
2025-10-09 13:25:25,487 - core.app - ERROR - Fatal error: unexpected indent (ct_research_runner.py, line 262)
Traceback (most recent call last):
  File "h:\Documents\LLM Code\AMP_LLMs\core\app.py", line 215, in run
    self.menu_system = MenuSystem(self)
                       ~~~~~~~~~~^^^^^^
  File "h:\Documents\LLM Code\AMP_LLMs\core\menu.py", line 46, in __init__
    self._register_menu_items()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "h:\Documents\LLM Code\AMP_LLMs\core\menu.py", line 55, in _register_menu_items
    from llm.ct_research_runner import run_ct_research_assistant
  File "h:\Documents\LLM Code\AMP_LLMs\llm\ct_research_runner.py", line 262
    if create.strip().lower() in ('n', 'no'):
IndentationError: unexpected indent
2025-10-09 13:25:25,489 - core.app - INFO - Closing SSH connection...
2025-10-09 13:25:25,489 - asyncssh - INFO - [conn=0] Closing connection
2025-10-09 13:25:25,489 - asyncssh - INFO - [conn=0] Sending disconnect: Disconnected by application (11)
2025-10-09 13:25:25,490 - asyncssh - INFO - [conn=0] Connection closed
2025-10-09 13:26:52,572 - core.app - INFO - Connected to 100.99.162.98
2025-10-09 13:26:55,580 - asyncssh - INFO - Host canonicalization disabled
2025-10-09 13:26:55,581 - asyncssh - INFO - Opening SSH connection to 100.99.162.98, port 22
2025-10-09 13:26:55,588 - asyncssh - INFO - [conn=0] Connected to SSH server at 100.99.162.98, port 22
2025-10-09 13:26:55,588 - asyncssh - INFO - [conn=0]   Local address: 100.69.245.72, port 53253
2025-10-09 13:26:55,588 - asyncssh - INFO - [conn=0]   Peer address: 100.99.162.98, port 22
2025-10-09 13:26:55,673 - asyncssh - INFO - [conn=0] Beginning auth for user emilyzhang
2025-10-09 13:26:55,859 - asyncssh - INFO - [conn=0] Auth for user emilyzhang succeeded
2025-10-09 13:26:55,859 - core.app - INFO - SSH connection established: emilyzhang@100.99.162.98
2025-10-09 13:26:55,972 - core.app - ERROR - Fatal error: unexpected indent (ct_research_runner.py, line 262)
Traceback (most recent call last):
  File "h:\Documents\LLM Code\AMP_LLMs\core\app.py", line 215, in run
    self.menu_system = MenuSystem(self)
                       ~~~~~~~~~~^^^^^^
  File "h:\Documents\LLM Code\AMP_LLMs\core\menu.py", line 46, in __init__
    self._register_menu_items()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "h:\Documents\LLM Code\AMP_LLMs\core\menu.py", line 55, in _register_menu_items
    from llm.ct_research_runner import run_ct_research_assistant
  File "h:\Documents\LLM Code\AMP_LLMs\llm\ct_research_runner.py", line 262
    if create.strip().lower() in ('n', 'no'):
IndentationError: unexpected indent
2025-10-09 13:26:55,975 - core.app - INFO - Closing SSH connection...
2025-10-09 13:26:55,975 - asyncssh - INFO - [conn=0] Closing connection
2025-10-09 13:26:55,975 - asyncssh - INFO - [conn=0] Sending disconnect: Disconnected by application (11)
2025-10-09 13:26:55,975 - asyncssh - INFO - [conn=0] Connection closed
2025-10-09 13:27:58,072 - core.app - WARNING - Failed to ping 100.99.162.98
2025-10-09 13:28:00,064 - core.app - WARNING - Failed to ping 100.99.162.98
2025-10-09 13:28:00,652 - core.app - INFO - Received signal 2, initiating graceful shutdown...
2025-10-09 13:28:14,673 - core.app - INFO - Connected to 100.99.162.98
2025-10-09 13:28:17,719 - asyncssh - INFO - Host canonicalization disabled
2025-10-09 13:28:17,719 - asyncssh - INFO - Opening SSH connection to 100.99.162.98, port 22
2025-10-09 13:28:17,741 - asyncssh - INFO - [conn=0] Connected to SSH server at 100.99.162.98, port 22
2025-10-09 13:28:17,742 - asyncssh - INFO - [conn=0]   Local address: 100.69.245.72, port 53290
2025-10-09 13:28:17,742 - asyncssh - INFO - [conn=0]   Peer address: 100.99.162.98, port 22
2025-10-09 13:28:17,829 - asyncssh - INFO - [conn=0] Beginning auth for user emilyzhang
2025-10-09 13:28:18,034 - asyncssh - INFO - [conn=0] Auth for user emilyzhang succeeded
2025-10-09 13:28:18,035 - core.app - INFO - SSH connection established: emilyzhang@100.99.162.98
2025-10-09 13:28:19,633 - core.menu - INFO - User selected: Clinical Trial Research Assistant
2025-10-09 13:28:22,427 - data.clinical_trial_rag - INFO - Building index from H:\Documents\LLM Code\AMP_LLMs\output\NCT05168774_method.json
2025-10-09 13:28:22,428 - data.clinical_trial_rag - INFO - Indexed 1 clinical trials
2025-10-09 13:28:22,430 - llm.async_llm_utils - INFO - Fetching models from http://100.99.162.98:11434/api/tags
2025-10-09 13:28:24,869 - llm.async_llm_utils - ERROR - Cannot connect to http://100.99.162.98:11434/api/tags - is Ollama running and accessible?
2025-10-09 13:28:25,887 - llm.async_llm_utils - INFO - Fetching models from http://localhost:11434/api/tags
2025-10-09 13:28:25,918 - llm.async_llm_utils - INFO - Found 13 models via API
2025-10-09 13:28:28,916 - llm.ct_research_runner - INFO - Created model ct-research-assistant from llama3:8b
2025-10-09 13:28:48,358 - llm.async_llm_utils - INFO - Sending 1420 characters to ct-research-assistant
2025-10-09 13:29:11,312 - llm.async_llm_utils - INFO - Received response: 1039 characters
2025-10-09 13:29:26,711 - llm.async_llm_utils - INFO - Sending 279 characters to ct-research-assistant
2025-10-09 13:29:47,654 - llm.async_llm_utils - INFO - Received response: 1766 characters
2025-10-09 13:30:02,349 - llm.async_llm_utils - INFO - Sending 227 characters to ct-research-assistant
2025-10-09 13:30:16,901 - llm.async_llm_utils - ERROR - Error sending prompt: Server disconnected
Traceback (most recent call last):
  File "h:\Documents\LLM Code\AMP_LLMs\llm\async_llm_utils.py", line 98, in send_to_ollama_api
    async with session.post(url, json=payload, timeout=aiohttp.ClientTimeout(total=120)) as resp:
               ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Documents\LLM Code\AMP_LLMs\llm_env\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "H:\Documents\LLM Code\AMP_LLMs\llm_env\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "H:\Documents\LLM Code\AMP_LLMs\llm_env\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "H:\Documents\LLM Code\AMP_LLMs\llm_env\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Documents\LLM Code\AMP_LLMs\llm_env\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.ServerDisconnectedError: Server disconnected
2025-10-09 13:30:19,304 - core.app - INFO - Received signal 2, initiating graceful shutdown...
2025-10-09 13:30:19,305 - core.app - INFO - Closing SSH connection...
2025-10-09 13:30:24,767 - core.app - INFO - Connected to 100.99.162.98
2025-10-09 13:30:27,692 - asyncssh - INFO - Host canonicalization disabled
2025-10-09 13:30:27,692 - asyncssh - INFO - Opening SSH connection to 100.99.162.98, port 22
2025-10-09 13:30:27,790 - asyncssh - INFO - [conn=0] Connected to SSH server at 100.99.162.98, port 22
2025-10-09 13:30:27,790 - asyncssh - INFO - [conn=0]   Local address: 100.69.245.72, port 53315
2025-10-09 13:30:27,790 - asyncssh - INFO - [conn=0]   Peer address: 100.99.162.98, port 22
2025-10-09 13:30:27,875 - asyncssh - INFO - [conn=0] Beginning auth for user emilyzhang
2025-10-09 13:30:28,063 - asyncssh - INFO - [conn=0] Auth for user emilyzhang succeeded
2025-10-09 13:30:28,064 - core.app - INFO - SSH connection established: emilyzhang@100.99.162.98
2025-10-09 13:30:30,062 - core.menu - INFO - User selected: Clinical Trial Research Assistant
2025-10-09 13:30:34,047 - data.clinical_trial_rag - INFO - Building index from H:\Documents\LLM Code\AMP_LLMs\output\NCT05168774_extended.json
2025-10-09 13:30:34,047 - data.clinical_trial_rag - INFO - Indexed 1 clinical trials
2025-10-09 13:30:34,049 - llm.async_llm_utils - INFO - Fetching models from http://100.99.162.98:11434/api/tags
2025-10-09 13:30:36,297 - llm.async_llm_utils - ERROR - Cannot connect to http://100.99.162.98:11434/api/tags - is Ollama running and accessible?
2025-10-09 13:30:37,305 - llm.async_llm_utils - INFO - Fetching models from http://localhost:11434/api/tags
2025-10-09 13:30:37,338 - llm.async_llm_utils - INFO - Found 13 models via API
2025-10-09 13:30:40,113 - llm.ct_research_runner - INFO - Created model ct-research-assistant from llama3:8b
2025-10-09 13:31:16,294 - llm.async_llm_utils - INFO - Sending 1483 characters to ct-research-assistant
2025-10-09 13:31:40,544 - llm.async_llm_utils - ERROR - Error sending prompt: Server disconnected
Traceback (most recent call last):
  File "h:\Documents\LLM Code\AMP_LLMs\llm\async_llm_utils.py", line 98, in send_to_ollama_api
    async with session.post(url, json=payload, timeout=aiohttp.ClientTimeout(total=120)) as resp:
               ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Documents\LLM Code\AMP_LLMs\llm_env\Lib\site-packages\aiohttp\client.py", line 1488, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "H:\Documents\LLM Code\AMP_LLMs\llm_env\Lib\site-packages\aiohttp\client.py", line 770, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "H:\Documents\LLM Code\AMP_LLMs\llm_env\Lib\site-packages\aiohttp\client.py", line 748, in _connect_and_send_request
    await resp.start(conn)
  File "H:\Documents\LLM Code\AMP_LLMs\llm_env\Lib\site-packages\aiohttp\client_reqrep.py", line 541, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Documents\LLM Code\AMP_LLMs\llm_env\Lib\site-packages\aiohttp\streams.py", line 680, in read
    await self._waiter
aiohttp.client_exceptions.ServerDisconnectedError: Server disconnected
2025-10-09 13:31:43,926 - llm.async_llm_utils - INFO - Sending 228 characters to ct-research-assistant
2025-10-09 13:31:46,237 - llm.async_llm_utils - ERROR - Cannot connect to http://localhost:11434/api/generate
2025-10-09 14:04:47,074 - core.app - INFO - Received signal 2, initiating graceful shutdown...
2025-10-09 14:04:47,075 - core.app - INFO - Closing SSH connection...
2025-10-09 15:35:09,063 - core.app - WARNING - Failed to ping 100.99.162.98
2025-10-09 15:35:10,064 - core.app - WARNING - Failed to ping 100.99.162.98
2025-10-09 15:35:11,079 - core.app - WARNING - Failed to ping 100.99.162.98
2025-10-09 15:35:12,072 - core.app - WARNING - Failed to ping 100.99.162.98
2025-10-09 15:35:13,062 - core.app - WARNING - Failed to ping 100.99.162.98
2025-10-09 15:35:14,066 - core.app - WARNING - Failed to ping 100.99.162.98
2025-10-09 15:35:15,068 - core.app - WARNING - Failed to ping 100.99.162.98
2025-10-09 15:35:16,068 - core.app - WARNING - Failed to ping 100.99.162.98
2025-10-09 15:35:17,093 - core.app - WARNING - Failed to ping 100.99.162.98
2025-10-09 15:35:18,070 - core.app - WARNING - Failed to ping 100.99.162.98
2025-10-09 15:35:19,067 - core.app - WARNING - Failed to ping 100.99.162.98
2025-10-09 15:35:33,573 - core.app - WARNING - Failed to ping 100.99.162.98
2025-10-09 15:35:35,066 - core.app - WARNING - Failed to ping 100.99.162.98
2025-10-09 15:35:36,076 - core.app - WARNING - Failed to ping 100.99.162.98
2025-10-09 15:37:25,714 - core.app - INFO - Received signal 2, initiating graceful shutdown...
2025-10-09 15:37:32,976 - core.app - INFO - Connected to 100.99.162.98
2025-10-09 15:37:36,588 - asyncssh - INFO - Host canonicalization disabled
2025-10-09 15:37:36,588 - asyncssh - INFO - Opening SSH connection to 100.99.162.98, port 22
2025-10-09 15:37:36,596 - asyncssh - INFO - [conn=0] Connected to SSH server at 100.99.162.98, port 22
2025-10-09 15:37:36,597 - asyncssh - INFO - [conn=0]   Local address: 100.69.245.72, port 64023
2025-10-09 15:37:36,597 - asyncssh - INFO - [conn=0]   Peer address: 100.99.162.98, port 22
2025-10-09 15:37:36,737 - asyncssh - INFO - [conn=0] Beginning auth for user emilyzhang
2025-10-09 15:37:36,934 - asyncssh - INFO - [conn=0] Auth for user emilyzhang succeeded
2025-10-09 15:37:36,935 - core.app - INFO - SSH connection established: emilyzhang@100.99.162.98
2025-10-09 15:37:38,738 - core.menu - INFO - User selected: Clinical Trial Research Assistant
2025-10-09 15:37:41,810 - data.clinical_trial_rag - INFO - Building index from H:\Documents\LLM Code\AMP_LLMs\output\NCT05168774_extended.json
2025-10-09 15:37:41,811 - data.clinical_trial_rag - INFO - Indexed 1 clinical trials
2025-10-09 15:37:41,813 - llm.async_llm_utils - INFO - Fetching models from http://100.99.162.98:11434/api/tags
2025-10-09 15:37:44,239 - llm.async_llm_utils - ERROR - Cannot connect to http://100.99.162.98:11434/api/tags - is Ollama running and accessible?
2025-10-09 15:37:45,251 - llm.async_llm_utils - INFO - Fetching models from http://localhost:11434/api/tags
2025-10-09 15:37:45,297 - llm.async_llm_utils - INFO - Found 13 models via API
2025-10-09 15:37:48,410 - llm.ct_research_runner - INFO - Created model ct-research-assistant from llama3:8b
2025-10-09 15:37:48,411 - llm.ct_research_runner - INFO - Started persistent Ollama session: http://localhost:11434
2025-10-09 15:38:29,322 - llm.ct_research_runner - INFO - Sending 1455 characters to ct-research-assistant
2025-10-09 15:38:31,621 - llm.ct_research_runner - ERROR - Cannot connect to Ollama at localhost:11434
2025-10-09 15:38:31,622 - llm.ct_research_runner - INFO - Closed persistent Ollama session
2025-10-09 15:38:33,627 - llm.ct_research_runner - INFO - Started persistent Ollama session: http://localhost:11434
2025-10-09 15:38:35,927 - llm.ct_research_runner - ERROR - Cannot connect to Ollama at localhost:11434
2025-10-09 15:38:35,927 - llm.ct_research_runner - INFO - Closed persistent Ollama session
2025-10-09 15:38:37,934 - llm.ct_research_runner - INFO - Started persistent Ollama session: http://localhost:11434
2025-10-09 15:38:40,234 - llm.ct_research_runner - ERROR - Cannot connect to Ollama at localhost:11434
2025-10-09 15:38:44,171 - llm.ct_research_runner - INFO - Sending 228 characters to ct-research-assistant
2025-10-09 15:38:46,465 - llm.ct_research_runner - ERROR - Cannot connect to Ollama at localhost:11434
2025-10-09 15:38:46,466 - llm.ct_research_runner - INFO - Closed persistent Ollama session
2025-10-09 15:38:48,473 - llm.ct_research_runner - INFO - Started persistent Ollama session: http://localhost:11434
2025-10-09 15:38:50,770 - llm.ct_research_runner - ERROR - Cannot connect to Ollama at localhost:11434
2025-10-09 15:38:50,770 - llm.ct_research_runner - INFO - Closed persistent Ollama session
2025-10-09 15:38:52,773 - llm.ct_research_runner - INFO - Started persistent Ollama session: http://localhost:11434
2025-10-09 15:38:55,076 - llm.ct_research_runner - ERROR - Cannot connect to Ollama at localhost:11434
2025-10-09 15:38:56,523 - llm.ct_research_runner - INFO - Closed persistent Ollama session
2025-10-09 15:38:59,626 - core.menu - INFO - User selected: Interactive Shell
2025-10-09 15:38:59,626 - core.app - WARNING - SSH connection lost, attempting reconnect...
2025-10-09 15:39:04,194 - core.app - INFO - Received signal 2, initiating graceful shutdown...
2025-10-09 15:39:04,194 - core.app - INFO - Closing SSH connection...
2025-10-09 15:39:10,565 - core.app - WARNING - Failed to ping 100.99.162.98
2025-10-09 15:39:12,081 - core.app - WARNING - Failed to ping 100.99.162.98
2025-10-09 15:39:13,068 - core.app - WARNING - Failed to ping 100.99.162.98
2025-10-09 15:41:18,065 - core.app - INFO - Connected to 100.99.162.98
2025-10-09 15:41:22,208 - asyncssh - INFO - Host canonicalization disabled
2025-10-09 15:41:22,208 - asyncssh - INFO - Opening SSH connection to 100.99.162.98, port 22
2025-10-09 15:41:22,252 - asyncssh - INFO - [conn=0] Connected to SSH server at 100.99.162.98, port 22
2025-10-09 15:41:22,252 - asyncssh - INFO - [conn=0]   Local address: 100.69.245.72, port 64070
2025-10-09 15:41:22,252 - asyncssh - INFO - [conn=0]   Peer address: 100.99.162.98, port 22
2025-10-09 15:41:22,335 - asyncssh - INFO - [conn=0] Beginning auth for user emilyzhang
2025-10-09 15:41:22,520 - asyncssh - INFO - [conn=0] Auth for user emilyzhang succeeded
2025-10-09 15:41:22,521 - core.app - INFO - SSH connection established: emilyzhang@100.99.162.98
2025-10-09 15:41:23,873 - core.menu - INFO - User selected: Clinical Trial Research Assistant
2025-10-09 15:41:26,898 - data.clinical_trial_rag - INFO - Building index from H:\Documents\LLM Code\AMP_LLMs\output\NCT05168774_method.json
2025-10-09 15:41:26,899 - data.clinical_trial_rag - INFO - Indexed 1 clinical trials
2025-10-09 15:41:26,901 - llm.async_llm_utils - INFO - Fetching models from http://100.99.162.98:11434/api/tags
2025-10-09 15:41:29,313 - llm.async_llm_utils - ERROR - Cannot connect to http://100.99.162.98:11434/api/tags - is Ollama running and accessible?
2025-10-09 15:41:30,327 - llm.async_llm_utils - INFO - Fetching models from http://localhost:11434/api/tags
2025-10-09 15:41:30,390 - llm.async_llm_utils - INFO - Found 13 models via API
2025-10-09 15:41:33,937 - llm.ct_research_runner - INFO - Created model ct-research-assistant from llama3:8b
2025-10-09 15:41:33,938 - llm.ct_research_runner - INFO - Started persistent Ollama session: http://localhost:11434
2025-10-09 15:42:34,896 - llm.ct_research_runner - INFO - Sending 1420 characters to ct-research-assistant
2025-10-09 15:42:37,199 - llm.ct_research_runner - ERROR - Cannot connect to Ollama at localhost:11434
2025-10-09 15:42:37,200 - llm.ct_research_runner - INFO - Closed persistent Ollama session
2025-10-09 15:42:39,207 - llm.ct_research_runner - INFO - Started persistent Ollama session: http://localhost:11434
2025-10-09 15:42:41,510 - llm.ct_research_runner - ERROR - Cannot connect to Ollama at localhost:11434
2025-10-09 15:42:41,510 - llm.ct_research_runner - INFO - Closed persistent Ollama session
2025-10-09 15:42:43,513 - llm.ct_research_runner - INFO - Started persistent Ollama session: http://localhost:11434
2025-10-09 15:42:45,810 - llm.ct_research_runner - ERROR - Cannot connect to Ollama at localhost:11434
2025-10-09 15:42:54,490 - llm.ct_research_runner - INFO - Sending 1455 characters to ct-research-assistant
2025-10-09 15:42:56,785 - llm.ct_research_runner - ERROR - Cannot connect to Ollama at localhost:11434
2025-10-09 15:42:56,785 - llm.ct_research_runner - INFO - Closed persistent Ollama session
2025-10-09 15:42:58,799 - llm.ct_research_runner - INFO - Started persistent Ollama session: http://localhost:11434
2025-10-09 15:43:01,100 - llm.ct_research_runner - ERROR - Cannot connect to Ollama at localhost:11434
2025-10-09 15:43:01,101 - llm.ct_research_runner - INFO - Closed persistent Ollama session
2025-10-09 15:43:03,103 - llm.ct_research_runner - INFO - Started persistent Ollama session: http://localhost:11434
2025-10-09 15:43:05,395 - llm.ct_research_runner - ERROR - Cannot connect to Ollama at localhost:11434
2025-10-09 15:43:09,249 - llm.ct_research_runner - INFO - Sending 1455 characters to ct-research-assistant
2025-10-09 15:43:11,546 - llm.ct_research_runner - ERROR - Cannot connect to Ollama at localhost:11434
2025-10-09 15:43:11,547 - llm.ct_research_runner - INFO - Closed persistent Ollama session
2025-10-09 15:43:13,551 - llm.ct_research_runner - INFO - Started persistent Ollama session: http://localhost:11434
2025-10-09 15:43:15,855 - llm.ct_research_runner - ERROR - Cannot connect to Ollama at localhost:11434
2025-10-09 15:43:15,855 - llm.ct_research_runner - INFO - Closed persistent Ollama session
2025-10-09 15:43:17,856 - llm.ct_research_runner - INFO - Started persistent Ollama session: http://localhost:11434
2025-10-09 15:43:20,159 - llm.ct_research_runner - ERROR - Cannot connect to Ollama at localhost:11434
2025-10-09 15:43:24,130 - llm.ct_research_runner - INFO - Sending 1455 characters to ct-research-assistant
2025-10-09 15:43:26,426 - llm.ct_research_runner - ERROR - Cannot connect to Ollama at localhost:11434
2025-10-09 15:43:26,427 - llm.ct_research_runner - INFO - Closed persistent Ollama session
2025-10-09 15:43:28,441 - llm.ct_research_runner - INFO - Started persistent Ollama session: http://localhost:11434
2025-10-09 15:43:30,749 - llm.ct_research_runner - ERROR - Cannot connect to Ollama at localhost:11434
2025-10-09 15:43:30,749 - llm.ct_research_runner - INFO - Closed persistent Ollama session
2025-10-09 15:43:32,758 - llm.ct_research_runner - INFO - Started persistent Ollama session: http://localhost:11434
2025-10-09 15:43:35,061 - llm.ct_research_runner - ERROR - Cannot connect to Ollama at localhost:11434
2025-10-09 15:43:49,530 - llm.ct_research_runner - INFO - Sending 1455 characters to ct-research-assistant
2025-10-09 15:43:51,841 - llm.ct_research_runner - ERROR - Cannot connect to Ollama at localhost:11434
2025-10-09 15:43:51,842 - llm.ct_research_runner - INFO - Closed persistent Ollama session
2025-10-09 15:43:53,845 - llm.ct_research_runner - INFO - Started persistent Ollama session: http://localhost:11434
2025-10-09 15:43:55,241 - core.app - INFO - Received signal 2, initiating graceful shutdown...
2025-10-09 15:43:55,242 - llm.ct_research_runner - INFO - Closed persistent Ollama session
2025-10-09 15:43:55,244 - core.app - INFO - Closing SSH connection...
2025-10-09 15:44:02,063 - core.app - WARNING - Failed to ping 100.99.162.98
2025-10-09 15:44:03,082 - core.app - WARNING - Failed to ping 100.99.162.98
2025-10-09 15:44:04,075 - core.app - WARNING - Failed to ping 100.99.162.98
2025-10-09 15:44:05,071 - core.app - WARNING - Failed to ping 100.99.162.98
2025-10-09 16:23:56,646 - core.app - INFO - Received signal 2, initiating graceful shutdown...
2025-10-09 16:42:42,737 - core.app - INFO - Connected to 100.99.162.98
2025-10-09 16:42:45,756 - asyncssh - INFO - Host canonicalization disabled
2025-10-09 16:42:45,756 - asyncssh - INFO - Opening SSH connection to 100.99.162.98, port 22
2025-10-09 16:42:45,773 - asyncssh - INFO - [conn=0] Connected to SSH server at 100.99.162.98, port 22
2025-10-09 16:42:45,773 - asyncssh - INFO - [conn=0]   Local address: 100.69.245.72, port 55227
2025-10-09 16:42:45,773 - asyncssh - INFO - [conn=0]   Peer address: 100.99.162.98, port 22
2025-10-09 16:42:45,869 - asyncssh - INFO - [conn=0] Beginning auth for user emilyzhang
2025-10-09 16:42:46,048 - asyncssh - INFO - [conn=0] Auth for user emilyzhang succeeded
2025-10-09 16:42:46,049 - core.app - INFO - SSH connection established: emilyzhang@100.99.162.98
2025-10-09 16:42:47,046 - core.menu - INFO - User selected: Clinical Trial Research Assistant
2025-10-09 16:42:53,117 - data.clinical_trial_rag - INFO - Building index from H:\Documents\LLM Code\AMP_LLMs\output\NCT05168774_method.json
2025-10-09 16:42:53,118 - data.clinical_trial_rag - INFO - Indexed 1 clinical trials
2025-10-09 16:42:53,119 - llm.async_llm_utils - INFO - Fetching models from http://100.99.162.98:11434/api/tags
2025-10-09 16:42:55,437 - llm.async_llm_utils - ERROR - Cannot connect to http://100.99.162.98:11434/api/tags - is Ollama running and accessible?
2025-10-09 16:42:56,447 - llm.async_llm_utils - INFO - Fetching models from http://localhost:11434/api/tags
2025-10-09 16:42:56,485 - llm.async_llm_utils - INFO - Found 13 models via API
2025-10-09 16:43:01,595 - llm.session_manager - INFO - Started persistent Ollama session: http://localhost:11434
2025-10-09 16:43:27,416 - llm.session_manager - INFO - Sending 1421 characters to ct-research-assistant
2025-10-09 16:43:41,720 - llm.session_manager - INFO - Received response: 582 characters
2025-10-09 16:44:17,924 - llm.session_manager - INFO - Sending 1421 characters to ct-research-assistant
2025-10-09 16:44:31,230 - llm.session_manager - WARNING - Attempt 1/3: Server disconnected
2025-10-09 16:44:31,230 - llm.session_manager - INFO - Closed persistent Ollama session
2025-10-09 16:44:33,239 - llm.session_manager - INFO - Started persistent Ollama session: http://localhost:11434
2025-10-09 16:44:35,539 - llm.session_manager - ERROR - Cannot connect to Ollama at localhost:11434
2025-10-09 16:44:35,540 - llm.session_manager - INFO - Closed persistent Ollama session
2025-10-09 16:44:37,541 - llm.session_manager - INFO - Started persistent Ollama session: http://localhost:11434
2025-10-09 16:44:39,835 - llm.session_manager - ERROR - Cannot connect to Ollama at localhost:11434
2025-10-09 16:44:51,811 - core.app - INFO - Received signal 2, initiating graceful shutdown...
2025-10-09 16:44:51,812 - llm.session_manager - INFO - Closed persistent Ollama session
2025-10-09 16:44:51,813 - core.app - INFO - Closing SSH connection...
2025-10-09 16:58:11,563 - core.app - WARNING - Failed to ping 100.99.162.98
2025-10-09 16:58:12,567 - core.app - WARNING - Failed to ping 100.99.162.98
2025-10-09 16:58:13,570 - core.app - WARNING - Failed to ping 100.99.162.98
2025-10-09 16:58:29,331 - core.app - INFO - Received signal 2, initiating graceful shutdown...
2025-10-09 16:58:41,062 - core.app - WARNING - Failed to ping 100.99.162.98
2025-10-09 16:58:42,564 - core.app - WARNING - Failed to ping 100.99.162.98
2025-10-09 17:23:21,098 - core.app - INFO - Received signal 2, initiating graceful shutdown...
